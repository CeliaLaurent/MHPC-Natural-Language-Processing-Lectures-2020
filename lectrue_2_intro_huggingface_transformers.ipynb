{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lectrue-2_intro-huggingface-transformers.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6KHAeyRvienh",
        "1Gf30V1GiUbB"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNOoTuwR4Jxnsddyso81b2t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/denocris/MHPC-Natural-Language-Processing-Lectures-2020/blob/master/lectrue_2_intro_huggingface_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8yh8e1CzbUY",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to the Transformers Library by Hugging Face (Lecture II)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7iPQnS5zlTW",
        "colab_type": "text"
      },
      "source": [
        "### My Contacts\n",
        "For any questions or doubts you can find my contacts here:\n",
        "\n",
        "* [Linkedin](https://www.linkedin.com/in/cristiano-de-nobili/) and [Twitter](https://twitter.com/denocris) (here I regulary post about AI and Science news)\n",
        "* My [Personal Website](https://denocris.com)\n",
        "* My [Instagram](https://www.instagram.com/denocris/?hl=it) (I am a Pilot, so here I mostly post about traveling, flying and adventures)\n",
        "* My recent TEDx on [AI and Human Creativity](https://youtu.be/8-hrmer9d_E)\n",
        "\n",
        "### Course Repository\n",
        "\n",
        "All notebooks can be found [here!](https://github.com/denocris/MHPC-Natural-Language-Processing-Lectures-2020)\n",
        "\n",
        "### Goals fo this lecture\n",
        "\n",
        "* Understanding the basics of Transformers library and its pipeline\n",
        "* Understanding of Transfer Learning, in particular the concept of fine-tuning\n",
        "* Warm-up and know about the most common problems in NLP\n",
        "\n",
        "In these lectures (and all the course) **we will never train a model from scratch**. This is beyond our possibilities at the moment. In this 2nd lecture, we will use pre-trained models and extract from them useful feautures (**feature-extraction**). Only during the 3rd lecture, we will **fine-tune a model**. So, keep in mind the differences between\n",
        "\n",
        "* Training from scratch (from random weights/parameters to learned ones). This is beyond our possibilities;\n",
        "\n",
        "* Feature-extraction (use without modify an already trained model as a generator of learned features);\n",
        "\n",
        "* Fine-tuninig (re-train on a specific task or dataset an already trained model)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlELp8dJPXn2",
        "colab_type": "text"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "514FFbX6hTx9",
        "colab_type": "text"
      },
      "source": [
        "[Transformers](https://huggingface.co/transformers/) was built by [Hugging Face](https://huggingface.co/), a Paris and NY startup whose mission is to democratize NLP for everyone. In the last year, they strongly contribute to the recent NLP revolution by building an easy to use interface between the latest models available and application to real cases.\n",
        "\n",
        "Transformers library provides state-of-the-art general-purpose transformer-based architectures (such as BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, T5, CTRL...) for Natural Language Processing, Understanding, and Generation with over thousands of pre-trained models in 100+ languages and deep interoperability between PyTorch & TensorFlow 2.0.\n",
        "\n",
        "Transformers is an opinionated library built for NLP researchers seeking to use/study/extend large-scale transformers models. The library was designed with two strong goals in mind:\n",
        "\n",
        "* be as easy and fast to use as possible;\n",
        "* provide state-of-the-art models with performances as close as possible to the original models.\n",
        "\n",
        "The aim of this section is to leverage the use of Transformers library pipelines API at the highest level possible. Without any training, we take advantage of pre-trained models to tackle a variety of downstream-tasks (Sentence Classification, Question & Answering). The idea is to warm-up with the most popular NLP problems.\n",
        "\n",
        "In the following lectures, we will dive into low-levels understanding in greater datails what is hidden in this high-level API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeEAIhtjhK5V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "c9bc6808-60fb-4cfd-b599-707e78ef3acd"
      },
      "source": [
        "!pip install -q transformers "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 675kB 4.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 21.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 49.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 44.0MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frLrD_L9hkMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import ipywidgets as widgets\n",
        "\n",
        "import numpy as np\n",
        "import copy\n",
        "import torch\n",
        "\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForQuestionAnswering"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGxxYMQbB4lo",
        "colab_type": "text"
      },
      "source": [
        "## About Transfer Learning, Pre-Trained Models and BERT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KHAeyRvienh",
        "colab_type": "text"
      },
      "source": [
        "**What Transfer Learning is?**\n",
        "\n",
        "Many NLP successful applications rely on transfer learning. The same is true also for Computer Vision and other deep learning fields.\n",
        "\n",
        "Transfer learning is a technique that consists to train a machine learning model for a task and use the knowledge gained in it to another different but related task.\n",
        "\n",
        "![alt text](https://capstoneretire.com/wp-content/uploads/2014/10/grandpa-granddaughter-walking.jpg)\n",
        "\n",
        "So why we should use Transfer Learning in NLP?\n",
        "\n",
        "* Many NLP tasks, such as question & answering or name entity recognition (NER), share common knowledge about language (underlying semantics…)\n",
        "\n",
        "* The opportunity to reuse the huge quantity of unlabeled texts from the web (in case of semi-unsupervised learning). \n",
        "\n",
        "The idea behind Transfer Learning is to try to store the knowledge gained in solving the source task in the source domain and apply it to another similar problem of interest is the same concept of the learning process by experience. We can learn something and we can use this knowledge to solve a similar task.\n",
        "\n",
        "Recent algorithms such as BERT-like ones (BERT, RoBERTa, GPT-n ecc...) have a huge number of parameters. To train them a lot of text data is required and a lot of energy power. So, for the moment, only big companies, well-founded startups, or research institutions can deal with this effort. So that is why we will not train from scratch one of this model. The good news is that many of the companies or institutions mentioned before, trained and will train models for us. That is why transfer learning is important! \n",
        "\n",
        "Regarding these models, let us have a bird-eye view of how they are trained.\n",
        "\n",
        "**But first, what a languange model is?**\n",
        "\n",
        "Language Modeling is the development of probabilistic models that predict a word in a sequence given the words. Example: \n",
        "\n",
        "`I <?> holidays` ( prob_love = 0.99, prob_hate = 0.009, prob_spaghetti = 0.001).\n",
        "\n",
        "**How BERT is trained from scratch?**\n",
        "\n",
        "A common practice in transfer learning (in NLP) is to train from scratch a language model in a semi-unsupervised manner (see the slide below). During training, 15% of tokens within each sentence are randomly selected. \n",
        "Then they are masked according to the following rule: \n",
        "\n",
        "- 80% of the time they are replaced with the MASK token\n",
        "(as shown below `I <mask> like a spritz`);\n",
        "- 10% with random token (`I peperoni like a spritz`);\n",
        "- 10% original token (`I would like a spritz`).\n",
        "\n",
        "<center>  <img src=\"https://docs.google.com/uc?export=download&id=1CUGSrqD6TPmojldiWY0enFganC7PjL8K\" width=\"600\" height=\"350\"> </center> \n",
        "\n",
        "BERT is also trained to solve another task in addition to language model: next-sentence prediction. Just to avoid to add to much information, I will skip to explain it in detail. It not relevant for our lecture. Just to mention, next-sentence prediction is just a binary classification that given two sentences it outputs if they are correlated (one is the next sentence of the other) or not.\n",
        "\n",
        "When the training is done and the pre-trained model released, this is when fine-tuning comes in action and we can use the model for a downstream task! One common workflow is to keep the pre-trained model internals unchanged adding more linear layers on top of a pre-trained model, or to use the model output as input to a separate model.\n",
        "\n",
        "<center>  <img src=\"https://docs.google.com/uc?export=download&id=1x5eBU67IdCiuQTOGHiZLn75_UDoRigR2\" width=\"600\" height=\"350\"> </center> \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "\n",
        "Transformers library provides many pre-trained models:\n",
        " * List of models pre-trained by Hugging Face ([look here](https://huggingface.co/transformers/pretrained_models.html))\n",
        "\n",
        " * List of models pre-trained and uploaded by the comunity ([here!](https://huggingface.co/models))\n",
        "\n",
        "Let us start importing this very special pre-trained model. This is simultaneously trained on 104 different languages. That is why it is called `multilingual`. Since a few months ago, it was the only model available trained also on less common languages. \n",
        "\n",
        "If you are working on English, I suggest you use a pre-trained model for English only such as `bert-base-cased`.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikCHMH5bK_mC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set multilingual pre-trained model\n",
        "pretrained_model = 'bert-base-multilingual-cased'\n",
        "\n",
        "# English only\n",
        "#pretrained_model = 'bert-base-cased'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvXPMHb6fcFb",
        "colab_type": "text"
      },
      "source": [
        "When a model has been chosen, the are two more steps to be done before starting everything with the library\n",
        "\n",
        "* Load pre-trained model tokenizer\n",
        "* load the pre-trained model (there are several possibilities, we will see later)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIeifEPqLDMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(pretrained_model)\n",
        "\n",
        "# Load model, in particular the base model with a head for masked language model\n",
        "model = BertForMaskedLM.from_pretrained(pretrained_model)\n",
        "\n",
        "# Set the model in evaluation mode\n",
        "model = model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Gf30V1GiUbB",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizers and Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kQ2mVZOT5t8",
        "colab_type": "text"
      },
      "source": [
        "A tokenizer is in charge of preparing the inputs for a model. In particular\n",
        "\n",
        "* tokenizing (splitting strings in sub-word token strings), converting tokens strings to ids and back, and encoding/decoding (i.e. tokenizing + convert to integers);\n",
        "\n",
        "* adding new tokens to the vocabulary in a way that is independent of the underlying structure (BPE, SentencePiece…);\n",
        "\n",
        "* managing special tokens like mask, beginning-of-sentence, etc tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTF13P4KIpCm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define sentences to be processed\n",
        "sentence_list = ['che tempo farà domani?',\n",
        "                 'hi, what is your name?',\n",
        "                 'sono stato al mercato a fare la spesa',\n",
        "                 'i do not trust artificial intelligence',\n",
        "                 'prenderò il treno per arrivare a trieste']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAYuSjEbKMrc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize sentences\n",
        "\n",
        "# Set print format\n",
        "fmt = '{:<8}{:<20}'\n",
        "# Set padding length\n",
        "padding_len = 20\n",
        "# Empty lists\n",
        "sentence_ids = []\n",
        "sentence_tokens = []\n",
        "\n",
        "# Loop over sentences\n",
        "for sentence in sentence_list:\n",
        "\n",
        "  # Tokenize (convert to dictionary IDs) and pad sentence adding special tokens at the beginning and at the end\n",
        "  sentence_ids.append(tokenizer.encode(sentence, add_special_tokens=True, max_length=padding_len, pad_to_max_length=True))\n",
        "\n",
        "  # Convert back IDs to string for visualization\n",
        "  sentence_tokens.append(tokenizer.convert_ids_to_tokens(sentence_ids[-1]))\n",
        "\n",
        "  # Print original sentence and its tokenized version\n",
        "  print('Original:', sentence)\n",
        "  print('-----------------')\n",
        "  print(fmt.format('ID', 'Token'))\n",
        "  print('-----------------')\n",
        "  for id, token in zip(sentence_ids[-1], sentence_tokens[-1]):\n",
        "    print(fmt.format(id, token))\n",
        "  print()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvhizfLENIjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print additional special tokens\n",
        "print(tokenizer.convert_ids_to_tokens(103), tokenizer.convert_ids_to_tokens(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00q3cOsiPAuI",
        "colab_type": "text"
      },
      "source": [
        "## BERT Language Model\n",
        "\n",
        "Here we are going to select one token for each sentence to be predicted by the pre-trained model. We will also generate the attention_mask. It helps when dealing with variance in the size of sequences and we need a way to tell the model that we don't want to attend to the padded indices of the sequence.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSO24w-IOJ29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set print format\n",
        "fmt = '{:<15}{:<15}{:<15}{:<15}{:<3}'\n",
        "\n",
        "# Input tokens to be masked (predicted)\n",
        "replace_tokens = ['tempo', 'your', 'mercato', 'artificial', 'treno']\n",
        "\n",
        "# Original sentences with masked tokens (deepcopy: It means that any changes made to a copy of object do not reflect in the original object. )\n",
        "input_ids = copy.deepcopy(sentence_ids)\n",
        "# Labels = original values of masked tokens\n",
        "label_ids = []\n",
        "# Mask for padding (0 for PAD token, 1 otherwise)\n",
        "attn_masks = []\n",
        "\n",
        "# Loop over sentences\n",
        "for i, tokens in enumerate(sentence_tokens):\n",
        "\n",
        "  # Find index of token to be replaced by MASK\n",
        "  ids_replace_token = tokens.index(replace_tokens[i])\n",
        "\n",
        "  # Replace token in original sentence with MASK\n",
        "  input_ids[i][ids_replace_token] = 103\n",
        "\n",
        "  # Store original value of masked token in labels, everything else is -1\n",
        "  label_ids.append([-100]*padding_len)\n",
        "  label_ids[i][ids_replace_token] = sentence_ids[i][ids_replace_token]\n",
        "\n",
        "  # Create attention mask (or padding mask)\n",
        "  attn_masks.append([0 if ids == 0 else 1 for ids in input_ids[i]])\n",
        "\n",
        "  # Print original and new (masked) sentence along with labels and padding masks\n",
        "  print('Original:', sentence_list[i])\n",
        "  print('---------------------------------------------------------------------')\n",
        "  print(fmt.format('ID', 'Token', 'Label ID', 'Label Token', 'Pad mask'))\n",
        "  print('---------------------------------------------------------------------')\n",
        "  for id, token, label_id, label_token, pad_mask in zip(input_ids[i], tokenizer.convert_ids_to_tokens(input_ids[i]), \n",
        "                                                        label_ids[i], tokenizer.convert_ids_to_tokens(label_ids[i]), attn_masks[i]):\n",
        "    print(fmt.format(id, token, label_id, label_token, pad_mask))\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJGsUkECOR3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert to pytorch tensors\n",
        "input_ids = torch.tensor(input_ids)\n",
        "label_ids = torch.tensor(label_ids)\n",
        "attn_masks = torch.tensor(attn_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0JkMNCSOUZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get predicted tokens with logits\n",
        "# model = BERTForMaskedLM\n",
        "outputs = model(input_ids=input_ids, attention_mask=attn_masks)\n",
        "predicted_logits = outputs[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smfOTTbipi-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outputs[0].size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMvgoCukOYkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get top k = 5 predicted tokens for each masked token\n",
        "\n",
        "# Set print format\n",
        "fmt = '{:<15}{:<15}'\n",
        "\n",
        "# Select k\n",
        "k = 5\n",
        "\n",
        "# Loop over sentences\n",
        "for i in range(predicted_logits.shape[0]):\n",
        "\n",
        "  # Find index of masked token within the sentence\n",
        "  masked_indexes = np.where(label_ids[i].numpy() != -100)\n",
        "  # Convert logits to probabilities for selected masked token\n",
        "  predicted_probs = torch.nn.functional.softmax(predicted_logits[i, masked_indexes], dim=2)\n",
        "  # Get top k probabilities and predicted tokens \n",
        "  predicted_topk_probs, predicted_topk_ids = torch.topk(predicted_probs, k=k, dim=2)\n",
        "\n",
        "  # Print original sentence and masked token (ground truth) vs top k predicted tokens and their probabilities\n",
        "  print('Original:', sentence_list[i])\n",
        "  print('Masked token:', replace_tokens[i])\n",
        "  print('---------------------------')\n",
        "  print(fmt.format('Prediction', 'Probability'))\n",
        "  print('---------------------------')\n",
        "  for token, probability in zip(tokenizer.convert_ids_to_tokens(predicted_topk_ids.view(-1).numpy()), \n",
        "                                [round(elem, 2) for elem in predicted_topk_probs.squeeze().tolist()]):\n",
        "    print(fmt.format(token, probability))\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DqZ-mnri5Ga",
        "colab_type": "text"
      },
      "source": [
        "## RoBERTa Language Model (Italian)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkNHQ4HEj1EC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import AutoModelWithLMHead\n",
        "\n",
        "# Set multilingual pre-trained model\n",
        "#pretrained_model = \"idb-ita/gilberto-uncased-from-camembert\"\n",
        "pretrained_model = \"Musixmatch/umberto-commoncrawl-cased-v1\"\n",
        "\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
        "model = AutoModelWithLMHead.from_pretrained(pretrained_model)\n",
        "model = model.eval()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoYVn265oQ9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define sentences to be processed\n",
        "sentence_list = ['che tempo farà domani?',\n",
        "                 'il mio cane è bello',\n",
        "                 'ho chiamato il medico ma non ha risposto',\n",
        "                 'il segreto di un buon caffè è la tazzina',\n",
        "                 'prenderò il treno per arrivare a trieste']\n",
        "\n",
        "# Tokenize sentences\n",
        "\n",
        "# Set print format\n",
        "fmt = '{:<8}{:<20}'\n",
        "# Set padding length\n",
        "padding_len = 20\n",
        "# Empty lists\n",
        "sentence_ids = []\n",
        "sentence_tokens = []\n",
        "\n",
        "# Loop over sentences\n",
        "for sentence in sentence_list:\n",
        "\n",
        "  # Tokenize (convert to dictionary IDs) and pad sentence adding special tokens at the beginning and at the end\n",
        "  sentence_ids.append(tokenizer.encode(sentence, add_special_tokens=True, max_length=padding_len, pad_to_max_length=True))\n",
        "\n",
        "  # Convert back IDs to string for visualization\n",
        "  sentence_tokens.append(tokenizer.convert_ids_to_tokens(sentence_ids[-1]))\n",
        "\n",
        "  # Print original sentence and its tokenized version\n",
        "  print('Original:', sentence)\n",
        "  print('-----------------')\n",
        "  print(fmt.format('ID', 'Token'))\n",
        "  print('-----------------')\n",
        "  for id, token in zip(sentence_ids[-1], sentence_tokens[-1]):\n",
        "    print(fmt.format(id, token))\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqiIYyHlsv6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set print format\n",
        "fmt = '{:<15}{:<15}{:<15}{:<15}{:<3}'\n",
        "\n",
        "# Input tokens to be masked (predicted)\n",
        "replace_tokens = ['▁tempo', '▁il', '▁chiamato', '▁di', '▁treno']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgeLQZnfQKEg",
        "colab_type": "text"
      },
      "source": [
        "We must check the special tokens for this tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ93CIPKQERb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.convert_ids_to_tokens([0,1,2,3,4,5,6,32004])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swPQPSJvsyyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Original sentences with masked tokens (deepcopy: It means that any changes made to a copy of object do not reflect in the original object. )\n",
        "input_ids = copy.deepcopy(sentence_ids)\n",
        "# Labels = original values of masked tokens\n",
        "label_ids = []\n",
        "# Mask for padding (0 for PAD token, 1 otherwise)\n",
        "attn_masks = []\n",
        "\n",
        "# Loop over sentences\n",
        "for i, tokens in enumerate(sentence_tokens):\n",
        "  ids_replace_token = tokens.index(replace_tokens[i])\n",
        "  print(ids_replace_token)\n",
        "    # Replace token in original sentence with MASK\n",
        "  input_ids[i][ids_replace_token] = 32004\n",
        "    # Store original value of masked token in labels, everything else is -3\n",
        "  label_ids.append([3]*padding_len)\n",
        "  label_ids[i][ids_replace_token] = sentence_ids[i][ids_replace_token]\n",
        "    # Create padding mask\n",
        "  attn_masks.append([0 if ids == 1 else 1 for ids in input_ids[i]])\n",
        "    # Print original and new (masked) sentence along with labels and padding masks\n",
        "  print('Original:', sentence_list[i])\n",
        "  print('---------------------------------------------------------------------')\n",
        "  print(fmt.format('ID', 'Token', 'Label ID', 'Label Token', 'Pad mask'))\n",
        "  print('---------------------------------------------------------------------')\n",
        "  for id, token, label_id, label_token, pad_mask in zip(input_ids[i], tokenizer.convert_ids_to_tokens(input_ids[i]), \n",
        "                                                        label_ids[i], tokenizer.convert_ids_to_tokens(label_ids[i]), attn_masks[i]):\n",
        "    print(fmt.format(id, token, label_id, label_token, pad_mask))\n",
        "  print()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxMM9q-40_bM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert to pytorch tensors\n",
        "input_ids = torch.tensor(input_ids)\n",
        "label_ids = torch.tensor(label_ids)\n",
        "attn_masks = torch.tensor(attn_masks)\n",
        "\n",
        "# Get predicted tokens with logits\n",
        "outputs = model(input_ids=input_ids, attention_mask=attn_masks)\n",
        "predicted_logits = outputs[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dPcuhxG0ybp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get top k = 5 predicted tokens for each masked token\n",
        "\n",
        "# Set print format\n",
        "fmt = '{:<15}{:<15}'\n",
        "\n",
        "# Select k\n",
        "k = 5\n",
        "\n",
        "# Loop over sentences\n",
        "for i in range(predicted_logits.shape[0]):\n",
        "\n",
        "  # Find index of masked token within the sentence\n",
        "  masked_indexes = np.where(label_ids[i].numpy() != 3)\n",
        "  # Convert logits to probabilities for selected masked token\n",
        "  predicted_probs = torch.nn.functional.softmax(predicted_logits[i, masked_indexes], dim=2)\n",
        "  # Get top k probabilities and predicted tokens \n",
        "  predicted_topk_probs, predicted_topk_ids = torch.topk(predicted_probs, k=k, dim=2)\n",
        "\n",
        "  # Print original sentence and masked token (ground truth) vs top k predicted tokens and their probabilities\n",
        "  print('Original:', sentence_list[i])\n",
        "  print('Masked token:', replace_tokens[i])\n",
        "  print('---------------------------')\n",
        "  print(fmt.format('Prediction', 'Probability'))\n",
        "  print('---------------------------')\n",
        "  for token, probability in zip(tokenizer.convert_ids_to_tokens(predicted_topk_ids.view(-1).numpy()), \n",
        "                                [elem for elem in predicted_topk_probs.squeeze().tolist()]):\n",
        "    print(fmt.format(token, probability))\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIhmsvT8UaS5",
        "colab_type": "text"
      },
      "source": [
        "## Pipelines (Transformers API)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xbJ43GUh1cA",
        "colab_type": "text"
      },
      "source": [
        "The aim of this section is to leverage the use of Transformers library `pipelines API` at the highest level possible. Without any training, we take advantage of pre-trained models to tackle a variety of downstream-tasks. \n",
        "\n",
        "In the next lecture, we will dive into a low-level understanding but for the moment let us leverage this high-level API. The idea is to warm-up with the most popular NLP problems.\n",
        "\n",
        "We will try the following downstream-tasks: \n",
        "\n",
        "- ***Sentence Classification _(Sentiment Analysis)_***: Indicate if the overall sentence is either positive or negative, i.e. *binary classification task* or *logitic regression task*.\n",
        "- ***Token Classification (Named Entity Recognition, Part-of-Speech tagging)***: For each sub-entities _(*tokens*)_ in the input, assign them a label, i.e. classification task.\n",
        "- ***Question-Answering***: Provided a tuple (`question`, `context`) the model should find the span of text in `content` answering the `question`.\n",
        "- ***Mask-Filling***: Suggests possible word(s) to fill the masked input with respect to the provided `context`.\n",
        "- ***Summarization***: Summarizes the ``input`` article to a shorter article.\n",
        "- ***Translation***: Translates the input from a language to another language.\n",
        "- ***Feature Extraction***: Maps the input to a higher, multi-dimensional space learned from the data.\n",
        "\n",
        "Pipelines encapsulate the overall process of every NLP process:\n",
        " \n",
        " 1. *Tokenization*: Split the initial input into multiple sub-entities (i.e. tokens).\n",
        " 2. *Inference*: Maps every tokens into a more meaningful representation. \n",
        " 3. *Decoding*: Use the above representation to generate and/or extract the final output for the underlying task.\n",
        "\n",
        "The overall API is exposed to the end-user through the `pipeline()` method with the following \n",
        "structure:\n",
        "\n",
        "```python\n",
        "from transformers import pipeline\n",
        "\n",
        "# Using custom model/tokenizer as str\n",
        "pipeline('<task-name>', model='<model name>', tokenizer='<tokenizer_name>')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbmWlbLRUhw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W19f3JakRil2",
        "colab_type": "text"
      },
      "source": [
        "### 1. Sentiment Analysis (English) \n",
        "\n",
        "Without any specification, pipeline set a default model for each task. Here a list of [default models](https://github.com/huggingface/transformers/blob/master/src/transformers/pipelines.py#L1459) given a task. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMwHG1eeS2EC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp_sentence_classif = pipeline('sentiment-analysis') \n",
        "# sst = Stanford Sentiment Treebank \n",
        "#nlp_sentence_classif = pipeline('sentiment-analysis', model = 'distilbert-base-uncased-finetuned-sst-2-english', tokenizer = 'distilbert-base-uncased')\n",
        "\n",
        "print(nlp_sentence_classif('It was a lovelly night !'))\n",
        "print(nlp_sentence_classif('That film is not at all worth seing'))\n",
        "print(nlp_sentence_classif('The event was pretty but it could be much better'))\n",
        "print(nlp_sentence_classif('He was kind last year but now I do not trust him'))\n",
        "print(nlp_sentence_classif('He is pretty ugly'))\n",
        "print(nlp_sentence_classif('He is pretty ugly but when I am with him I am really happy'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h2xb6o_y6HB",
        "colab_type": "text"
      },
      "source": [
        "### 2. Token Classification and Name Entity Recognition (NER)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDg2sU1AWg9t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp_token_class = pipeline('ner')\n",
        "nlp_token_class('Trieste, where SISSA is located, is a beautiful city in Italy, rich of science and sea')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV3x2-hl2x9y",
        "colab_type": "text"
      },
      "source": [
        "### 3. Question and Answering (Q&A)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5A3Z0s9zqqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp_qa = pipeline('question-answering')\n",
        "nlp_qa(context='Trieste, where SISSA is located, is a beautiful city in Italy, rich of science and sea', question='Where is SISSA ?')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ve692bP3Bnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp_qa(context='I left my keys at home. I cannot find them in my bag', question='Where are my keys ?')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hg8rLq4i314C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp_qa(context='I cannot find my keys. I left them at home', question='Where are my keys ?')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dp2apZ9M4YXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/bert-italian-finedtuned-squadv1-it-alfa\")\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"mrm8488/bert-italian-finedtuned-squadv1-it-alfa\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7Cy4tEk4hPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp_qa_ita = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
        "nlp_qa_ita(context='Abito da anni a Trieste e mi trovo molto bene', question='dove vivo ?')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "allLLUt15CpU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp_qa_ita(context='Non trovo le mie chiavi nella borsa, saranno a casa !', question='dove sono le chiavi ?')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOAVEhdG5TU6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp_qa_ita(context='Camilla è figlia di mio padre Umberto e mia madre Isabella', question='Come si chiama mia sorella ?')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae6vEobs6paJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp_qa_ita(context='Mio padre Umberto e mia madre Isabella hanno una figlia di nome Camilla', question='Come si chiama mia sorella ?')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIDNWWyf62JZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp_qa_ita(context='Mio padre di nome Umberto e mia madre di nome Isabella hanno una figlia di nome Camilla', question='Come si chiama mia mamma ?')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gMHft7j5nrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp_qa_ita(context='Udine è bella ma Trieste lo è ancora di più', question='Quale città è più bella ?')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR60MS498AJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp_qa_ita(context='Trieste è stupenda ma anche Udine è da visitare ', question='Quale città è più bella ?')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VUAcP7R8Pw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp_qa_ita(context='Trieste è stupenda e Udine è più tranquilla ', question='Quale città è più tranquilla?')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekzS8JBA6PVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp_qa_ita(context='Mio padre Umberto ha una figlia Camilla con mia madre Isabella', question='Chi è mia madre?')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcObDqvT00bK",
        "colab_type": "text"
      },
      "source": [
        "### 4. Mask-filling\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Djbodn3LHrt9",
        "colab_type": "text"
      },
      "source": [
        "In English..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R6iKEg21CNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp_fill = pipeline('fill-mask')\n",
        "nlp_fill('Hugging Face is a' + nlp_fill.tokenizer.mask_token + ' company based in Paris' )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e40bLutX4-pZ",
        "colab_type": "text"
      },
      "source": [
        "In Italian..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHmN3Xj01XZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Musixmatch/umberto-commoncrawl-cased-v1\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"Musixmatch/umberto-commoncrawl-cased-v1\")\n",
        "\n",
        "nlp_fill = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n",
        "nlp_fill('Dopo lavoro ci vediamo tutti per un ' + nlp_fill.tokenizer.mask_token)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSXexFmF4S5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp_fill = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n",
        "nlp_fill('Prendo il ' + nlp_fill.tokenizer.mask_token + ' così andiamo al mare')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fbs9t1KvrzDy",
        "colab_type": "text"
      },
      "source": [
        "### 5. Summarization\n",
        "\n",
        "Summarization is currently supported by `Bart` and `T5`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BaOgzi1u1Yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT_TO_SUMMARIZE = \"\"\" \n",
        "New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York. \n",
        "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband. \n",
        "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other. \n",
        "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage. \n",
        "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the \n",
        "2010 marriage license application, according to court documents. \n",
        "Prosecutors said the marriages were part of an immigration scam. \n",
        "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further. \n",
        "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective \n",
        "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002. \n",
        "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say. \n",
        "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages. \n",
        "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted. \n",
        "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s \n",
        "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali. \n",
        "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force. \n",
        "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
        "\"\"\"\n",
        "\n",
        "summarizer = pipeline('summarization')\n",
        "summarizer(TEXT_TO_SUMMARIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5JA6IJsr-G0",
        "colab_type": "text"
      },
      "source": [
        "### 6. Translation\n",
        "\n",
        "Translation is currently supported by `T5` for the language mappings English-to-French (`translation_en_to_fr`), English-to-German (`translation_en_to_de`) and English-to-Romanian (`translation_en_to_ro`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FwayP4nwV3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# English to French\n",
        "translator = pipeline('translation_en_to_fr')\n",
        "translator(\"HuggingFace is a French company that is based in New York City. HuggingFace's mission is to solve NLP one commit at a time\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ra0-WfznwoIW",
        "colab": {}
      },
      "source": [
        "# English to German\n",
        "translator = pipeline('translation_en_to_de')\n",
        "translator(\"The history of natural language processing (NLP) generally started in the 1950s, although work can be found from earlier periods.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPUpg0M8hCtB",
        "colab_type": "text"
      },
      "source": [
        "### 7. Text Generation\n",
        "\n",
        "Text generation is currently supported by GPT-2, OpenAi-GPT, TransfoXL, XLNet, CTRL and Reformer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pKfxTxohXuZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_generator = pipeline(\"text-generation\")\n",
        "text_generator(\"Today is a beautiful day and I will\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Utmldmetrl_9",
        "colab_type": "text"
      },
      "source": [
        "### 8. Features Extraction and Attention Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%% code\n"
        },
        "id": "O4SjR1QQrl__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp_features = pipeline('feature-extraction')\n",
        "output = nlp_features('Hugging Face is a French company based in Paris')\n",
        "np.array(output).shape   # (Samples, Tokens, Vector Size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vZlMd_VoTc8",
        "colab_type": "text"
      },
      "source": [
        "Credits to [BertViz repo](https://github.com/jessevig/bertviz) by [Jesse Vig](https://twitter.com/jesse_vig): BertViz is a tool for visualizing attention in the Transformer model, supporting all models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKZ9yeI0HUDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "!test -d bertviz_repo || git clone https://github.com/jessevig/bertviz bertviz_repo\n",
        "if not 'bertviz_repo' in sys.path:\n",
        "  sys.path += ['bertviz_repo']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8P61wVDGvWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bertviz import head_view,  model_view"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgplwHMzGwva",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def call_html():\n",
        "  import IPython\n",
        "  display(IPython.core.display.HTML('''\n",
        "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "        <script>\n",
        "          requirejs.config({\n",
        "            paths: {\n",
        "              base: '/static/base',\n",
        "              \"d3\": \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.8/d3.min\",\n",
        "              jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
        "            },\n",
        "          });\n",
        "        </script>\n",
        "        '''))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaMNydm9HpfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "model_version = 'bert-base-uncased'\n",
        "do_lower_case = True\n",
        "\n",
        "model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)\n",
        "\n",
        "#sentence_a = \"Attention is important, we need to understand it necessarily\"\n",
        "sentence_a = \"he is going to take his train to milan quite soon\"\n",
        "\n",
        "\n",
        "inputs = tokenizer.encode_plus(sentence_a, return_tensors='pt', add_special_tokens=False)\n",
        "token_type_ids = inputs['token_type_ids']\n",
        "input_ids = inputs['input_ids']\n",
        "attention = model(input_ids, token_type_ids=token_type_ids)[-1]\n",
        "input_id_list = input_ids[0].tolist() # Batch index 0\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
        "call_html()\n",
        "\n",
        "head_view(attention, tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9HH1-cKJvVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def call_html():\n",
        "  import IPython\n",
        "  display(IPython.core.display.HTML('''\n",
        "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "        <script>\n",
        "          requirejs.config({\n",
        "            paths: {\n",
        "              base: '/static/base',\n",
        "              \"d3\": \"https://cdnjs.cloudflare.com/ajax/libs/d3/5.7.0/d3.min\",\n",
        "              jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
        "            },\n",
        "          });\n",
        "        </script>\n",
        "        '''))\n",
        "\n",
        "call_html()\n",
        "\n",
        "model_view(attention, tokens)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}