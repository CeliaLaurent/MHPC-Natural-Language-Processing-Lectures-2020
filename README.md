# Natural Language Processing 

This is the second part of the Deep Learning Course for the [Master in High-Performance Computing](https://twitter.com/mhpc_sissa_ictp) (SISSA/ICTP). Part of the course was supported by [AINDO](https://www.linkedin.com/company/aindo/)

Cristiano holds a Ph.D. in Theoretical Physics ([SISSA](https://twitter.com/Sissaschool)) and he has been actively working in Deep Learning for four years. In particular, he is now part of the Bixby project, Samsung's vocal assistant. He is also a TEDx speaker (here is talk about [AI, Humans and their future](https://youtu.be/8-hrmer9d_E)) and civil pilot (PPL). Here his contacts:

* If you are interested in science and tech news: [LinkedIn](https://www.linkedin.com/in/cristiano-de-nobili/) & [Twitter](https://twitter.com/denocris);
* On my [website](https://denocris.com/) you can find all my lectures, workshops, and talks;
* My [Instagram](https://www.instagram.com/denocris/?hl=it) is about flying, traveling, and adventure. It is the social platform that I use the most.


## Course Outline

* Lecture 1: intro to NLP, text preprocessing, spaCy, common problems in NLP (NER, POS, sentence classification, ecc... ), non-contextual word embedding, SkipGram Word2Vec coded from scratch, pre-trained Glove with Gensim, intro to contextual word embedding and (self-)Attention Mechanism.

* Lecture 2: transfer learning main concepts, transformer-based model, how BERT-like models are trained and fine-tuned on downstream tasks, intro to Transformers library, tokenization, language modeling with English and non-English (Italian Gilberto and Umberto) pre-trained AutoModels, some examples of NLP problems using Transformers Pipeline.

* Lecture 3: fine-tune a pre-trained Italian RoBERTa to solve word-sense disambiguation, embedding geometry, clustering (TSNE and UMAP) and visualization (this lecture is a bit advanced).


For any doubts or questions feel free to contact me!

