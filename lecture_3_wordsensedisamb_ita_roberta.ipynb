{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lecture-3_wordsensedisamb-ita-roberta.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KfiAkxDXIjPv"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/denocris/MHPC-Natural-Language-Processing-Lectures-2020/blob/master/lecture_3_wordsensedisamb_ita_roberta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tXAvx0eL44RK"
      },
      "source": [
        "# Word Sense Disambiguation and Embedding Visualization with an Italian RoBERTa\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGb4J9GgA1Xm",
        "colab_type": "text"
      },
      "source": [
        "### Slides\n",
        "To follow the notebook [these slides](https://docs.google.com/presentation/d/19c6Go-jc9yH5MZceHxPJC1VPsXgCp2Rb_FVTQZB01Q8/edit?usp=sharing) might be useful. \n",
        "\n",
        "### My Contacts\n",
        "For any questions or doubts you can find my contacts here:\n",
        "\n",
        "* [Linkedin](https://www.linkedin.com/in/cristiano-de-nobili/) and [Twitter](https://twitter.com/denocris) (here I post about AI and Science)\n",
        "* My recent TEDx on [AI and Human Creativity](https://youtu.be/8-hrmer9d_E)\n",
        "* My [Personal Website](https://denocris.com)\n",
        "* My [Instagram](https://www.instagram.com/denocris/?hl=it) (I am a Pilot, so here I mostly post about my adventures)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sre6L325A70f",
        "colab_type": "text"
      },
      "source": [
        "### Goals fo this lecture\n",
        "\n",
        "* Going deeper with Transformers library: fine-tune a model to solve a downstream task (Classification Task)\n",
        "* Working with non-English data and models\n",
        "* Taste PyTorch Lightninig\n",
        "* Embedding Visualization and Clustering\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTldaRmfLWeT",
        "colab_type": "text"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "#### What is BERT ?\n",
        "\n",
        "[BERT](https://arxiv.org/abs/1810.04805) (Bidirectional Encoder Representations from Transformers) is a [transformer-based architecture](https://arxiv.org/abs/1706.03762). More precisely, it is a stack of **transformer** encoder layers that consist of multiple heads, i.e. fully connected neural networks augmented with a self-attention mechanism. \n",
        "\n",
        "**Self-attention** is a non-local operator, which means that at any layer a token can attend to all other tokens regardless of their distance. Self-attention thus produces so-called contextual word embeddings, as successive layers gradually aggregate contextual information into the embedding of the input word.\n",
        "\n",
        "<center>  <img src=\"https://drive.google.com/uc?id=1fdBEGUc4mfXTbyaUVnTEZQa6Zwj0ZTnT\" width=\"650\" height=\"400\">  </center> \n",
        "\n",
        "The input to BERT is based on a sequence of tokens (words or pieces of words). The output is a sequence of vectors, one for each input token. We will often refer to these vectors as context embeddings because they include information about a token’s context. \n",
        "\n",
        "The conventional workflow for BERT consists of two stages: **pre-training** and **fine-tuning** for downstream applications. In this notebook, we will deal with fine-tuning. In fine-tuning, one or more layers are typically added on top of the pre-trained BERT. The whole algorithm will then be fine-tuned on some data.\n",
        "\n",
        "#### What will we do?\n",
        "\n",
        "In our specific case, we will use [RoBERTa](https://arxiv.org/abs/1907.11692). It is a variant of of BERT which \"modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates\".\n",
        "\n",
        "In particular, we will start from an Italian pre-trained RoBERTA (Gilberto and Umberto) and fine-tune it to a classification downstream task. In particular, as a simple example, we `will disambiguate a polysemic Italian word`.\n",
        "\n",
        "How it is possible disambiguation? BERT generates contextual embeddings. \n",
        "\n",
        "Experiments suggest context embeddings in BERT and related models contain enough information to perform many tasks in the traditional NLP pipeline. In particular, the authors of [this paper](https://arxiv.org/abs/1906.02715), explore the geometry of internal representations of BERT and claim that its contextual embeddings contains information about syntactics and semantics of a word. In particular, turning to semantics, using visualizations of the activations created by different pieces of text, they show suggestive evidence that BERT distinguishes word senses at a very ﬁne level.\n",
        "\n",
        "\n",
        "Actually, here we will work with RoBERTa (which is a slight modification of the original BERT in particular in the way of training) and following the procedure explained in the above paper we will at the end visualize word embeddings.\n",
        "\n",
        "---\n",
        "\n",
        "### Tools Used\n",
        "\n",
        "* [PyTorch](https://pytorch.org/) as deep learning framework\n",
        "* [HuggingFace Transformers](https://huggingface.co/transformers/) library as NLP tool kit\n",
        "* [Italian Gilberto](https://github.com/idb-ita/GilBERTo) pre-trained RoBERTa by Giulio Ravasio and Leonardo Di Perna.\n",
        "* [Italian Umberto](https://github.com/musixmatchresearch/umberto) pre-trained RoBERTa by Simone Francia, Loreto Parisi, Paolo Magnani (Musixmatch).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Some Refs on the Geometry of BERT\n",
        "\n",
        "We will not be mathematically precise about the geometry of BERT embeddings. They have been recently discovered and many studies are on going. Some interesting papers: [one](https://nlp.stanford.edu/pubs/hewitt2019structural.pdf), [two](https://arxiv.org/abs/1906.02715), [three](https://arxiv.org/abs/1909.00512), [four](https://arxiv.org/abs/2002.12327).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9D0U5i8j69d",
        "colab_type": "text"
      },
      "source": [
        "## Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YckwuEWQjVbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Note need to Restart runtime after running this code segment\n",
        "%%capture\n",
        "!git clone https://github.com/davidtvs/pytorch-lr-finder.git && cd pytorch-lr-finder && python setup.py install"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhnnpxsqX6VR",
        "colab_type": "code",
        "outputId": "19af3b93-9c77-4257-d64a-91f2ce8b84ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "!pip install -q transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 675kB 2.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 14.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 20.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 23.1MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhJR0wZ-Dggq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt \n",
        "import random\n",
        "import copy \n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjNN4C5bDmAF",
        "colab_type": "code",
        "outputId": "3ab749fa-9d31-4e63-c03c-361e8941d617",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data import Dataset\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from transformers import BertTokenizer, AutoTokenizer, DistilBertTokenizer\n",
        "from transformers import BertModel, AutoModel, DistilBertModel\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRYrCB6w7uJH",
        "colab_type": "text"
      },
      "source": [
        "Check GPU Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtpvA2n07qL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK1nBwL47tFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime → \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7ZglTGtDq1c",
        "colab_type": "text"
      },
      "source": [
        "## Download Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QTklu0esEOw",
        "colab_type": "text"
      },
      "source": [
        "For lack of time, we omit data preprocessing and just dowload some ready-to-use data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBzkFe3erpbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download Training Set\n",
        "!gdown --id 1cYJCe2eIqDLZWPzWxpjcVU4AQbmunQvQ\n",
        "# Download Validation Set\n",
        "!gdown --id 1aL7uZ5FEdWjUe4hqZD9kzJgC8-IWd-0-\n",
        "# Download Test Set\n",
        "!gdown --id 1Q1AGMgUD3XH89G2XLODmBOJ6ybZyyjFk\n",
        "# Mkdir Model\n",
        "!mkdir Models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wP3kgj_Osykc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wc -l *_sei.tsv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zeAaS5I83Ft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! head -n 5000 train_sei.tsv > train_sei_short.tsv\n",
        "! head -n 1000 valid_sei.tsv > valid_sei_short.tsv\n",
        "! head -n 500 test_sei.tsv > test_sei_short.tsv\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0QzdONbs8WE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! head -n 10 train_sei.tsv\n",
        "# label 1: sei VERB\n",
        "# label 0: sei NUMBER"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d1UsaKeWlrI",
        "colab_type": "text"
      },
      "source": [
        "As you can see, our dataset is made of two type of sentences (labelled differently): sentences where the word `sei` is a VERB and sentences where sei is a NUMBER. The dataset was built scraping different sources from the web. Only sentences shorter than a fixed length were filtered.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsYv4GxvNAUs",
        "colab_type": "text"
      },
      "source": [
        "### Data Loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3UqqD2J3trq",
        "colab_type": "text"
      },
      "source": [
        "Before giving any inputs to a BERT-like model, it is important to prepare the sentence the way BERT likes to ingest it. In more technical words, we must tokenize properly our input. A tokenizer is in charge of preparing the inputs for a model. We will use pre-trained tokenizer, but it is worth knowing that you can train your own tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTkoTujhM1kK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SSTDataset(Dataset):\n",
        "\n",
        "    def __init__(self, filename, maxlen, bert_type = 'ita-gilberto', pyt_lightning = False):\n",
        "\n",
        "        #Store the contents of the file in a pandas dataframe\n",
        "        self.df = pd.read_csv(filename, delimiter = '\\t', header=None)\n",
        "        self.maxlen = maxlen\n",
        "        self.pyt_lightning = pyt_lightning\n",
        "\n",
        "        #Initialize the BERT tokenizer\n",
        "        if bert_type == 'ita-gilberto':\n",
        "            # Italian BERT (GilBERTo)\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\"idb-ita/gilberto-uncased-from-camembert\", do_lower_case=True)\n",
        "        elif bert_type == 'ita-umberto':\n",
        "            # Italian BERT (UmBERTo)\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\"Musixmatch/umberto-commoncrawl-cased-v1\", do_lower_case=True)\n",
        "        elif bert_type == 'multilingual':\n",
        "            # Multilingual\n",
        "            self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True)\n",
        "        elif bert_type == 'distil-multilingual': \n",
        "            # Distilled Multilingual\n",
        "            self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased', do_lower_case=True)\n",
        "\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        #Selecting the sentence and label at the specified index in the data frame\n",
        "        sentence = self.df.loc[index, 0]\n",
        "        label = self.df.loc[index, 1]\n",
        "\n",
        "        # tokenize the sentence\n",
        "        try:\n",
        "            encoded_sent = self.tokenizer.encode(sentence, add_special_tokens = True)\n",
        "        except:\n",
        "            print(\"---- CHECK THIS SENTENCE ----- >>\", sentence, label)\n",
        "            sys.exit()\n",
        "\n",
        "        # padding the sentence to the max length\n",
        "        if len(encoded_sent) > self.maxlen:\n",
        "            encoded_sent = encoded_sent[:self.maxlen - 2]\n",
        "        padded_sent  = encoded_sent + [0 for _ in range(self.maxlen - len(encoded_sent))]\n",
        "        #Converting the list to a pytorch tensor\n",
        "        tokens_ids_tensor = torch.tensor(padded_sent) #Converting the list to a pytorch tensor\n",
        "        #Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
        "        attn_mask = (tokens_ids_tensor != 0).long()\n",
        "\n",
        "        label = torch.tensor(label, dtype=torch.float)\n",
        "\n",
        "        if self.pyt_lightning == True:\n",
        "          return (tokens_ids_tensor, attn_mask), label\n",
        "        else:\n",
        "          return tokens_ids_tensor, attn_mask, label\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UGnLt35yGyJ",
        "colab_type": "text"
      },
      "source": [
        "Let us unroll the funcrion and see it step-by-step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1NKZDofxPV-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Choose a tokenizer, let's say Gilberto\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"idb-ita/gilberto-uncased-from-camembert\", do_lower_case=True)\n",
        "\n",
        "#Choose a sentence\n",
        "sentence = 'Ciao come stai? Cosa fai stasera?'\n",
        "\n",
        "# Tokenize with special tokens (<s> and <\\s> for RoBERTa)\n",
        "encoded_sent = tokenizer.encode(sentence, add_special_tokens = True)\n",
        "tokens_from_ids = tokenizer.convert_ids_to_tokens(encoded_sent)\n",
        "padded_sent  = encoded_sent + [0 for _ in range(20 - len(encoded_sent))]\n",
        "tokens_ids_tensor = torch.tensor(padded_sent)\n",
        "attn_mask = (tokens_ids_tensor != 0).long()\n",
        "\n",
        "print('\\n', encoded_sent)\n",
        "print('\\n', tokens_from_ids)\n",
        "print('\\n', padded_sent)\n",
        "print('\\n', tokens_ids_tensor)\n",
        "print('\\n', attn_mask)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXbmP9mQNye1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAXLEN= 50\n",
        "bert_type = 'ita-gilberto' #'ita-umberto'\n",
        "BATCH_SIZE = 8\n",
        "MAX_EPS = 5\n",
        "\n",
        "#Creating dataloaders\n",
        "train_set = SSTDataset(filename = './train_sei_short.tsv', maxlen = MAXLEN, bert_type = bert_type)\n",
        "val_set = SSTDataset(filename = './valid_sei_short.tsv', maxlen = MAXLEN, bert_type = bert_type)\n",
        "test_set = SSTDataset(filename = './test_sei_short.tsv', maxlen = MAXLEN, bert_type = bert_type)\n",
        "\n",
        "# Data loader combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
        "train_loader = DataLoader(train_set, batch_size = BATCH_SIZE , num_workers = 2)\n",
        "val_loader = DataLoader(val_set, batch_size = BATCH_SIZE , num_workers = 2)\n",
        "test_loader = DataLoader(test_set, batch_size = BATCH_SIZE , num_workers = 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4yvCYCBMvSS",
        "colab_type": "text"
      },
      "source": [
        "## Model Definition and Training\n",
        "\n",
        "Before starting we want to better understand the different ways to import our models. What is the difference between AutoModel (or BertModel) and AutoModelWithLMHead (or BertModelWithLMHead)?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJpqZrkVCmnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import AutoModelWithLMHead\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"idb-ita/gilberto-uncased-from-camembert\", do_lower_case=True)\n",
        "# base model\n",
        "model = AutoModel.from_pretrained(\"idb-ita/gilberto-uncased-from-camembert\")\n",
        "# base model with LM Head\n",
        "model_lmh = AutoModelWithLMHead.from_pretrained(\"idb-ita/gilberto-uncased-from-camembert\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k0G4lv9DYyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"Milano si trova in lombardia\"\n",
        "enc = tokenizer.encode_plus(text)\n",
        "enc.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWqppqk0EhK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(enc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8_s1e5vEW8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out = model(torch.tensor(enc[\"input_ids\"]).unsqueeze(0), torch.tensor(enc[\"attention_mask\"]).unsqueeze(0))\n",
        "out[0].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-diYCOugEtMq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out = model_lmh(torch.tensor(enc[\"input_ids\"]).unsqueeze(0), torch.tensor(enc[\"attention_mask\"]).unsqueeze(0))\n",
        "out[0].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5hGG896d864",
        "colab_type": "text"
      },
      "source": [
        "We can also do..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOWBpeRHE9R4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_model = model.base_model\n",
        "out = base_model(torch.tensor(enc[\"input_ids\"]).unsqueeze(0), torch.tensor(enc[\"attention_mask\"]).unsqueeze(0))\n",
        "out[0].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am6C1R_6FKA7",
        "colab_type": "text"
      },
      "source": [
        "### Models Definition\n",
        "\n",
        "Let's define here the simplest BERT model possible: BERT with a classification head"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tumiYfWNtNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    # -------------------------\n",
        "    # Italian pre-trained Model with a binary classification head.\n",
        "    # -------------------------\n",
        "    def __init__(self, freeze_bert = True, bert_type = 'ita-gilberto'):\n",
        "        super(Net, self).__init__()\n",
        "        #Instantiating BERT model object\n",
        "        self.bert_type = bert_type \n",
        "        if bert_type == 'ita-gilberto':\n",
        "            # Italian BERT (GilBERTo)\n",
        "            self.bert_layer = AutoModel.from_pretrained(\"idb-ita/gilberto-uncased-from-camembert\")\n",
        "        elif bert_type == 'ita-umberto':\n",
        "            # Italian BERT (UmBERTo)\n",
        "            self.bert_layer = AutoModel.from_pretrained(\"Musixmatch/umberto-commoncrawl-cased-v1\")\n",
        "        elif bert_type == 'multilingual':\n",
        "            # Multilingual\n",
        "            self.bert_layer = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "        elif bert_type == 'distil-multilingual': \n",
        "          # Distilled Multilingual\n",
        "          self.bert_layer = DistilBertModel.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
        "\n",
        "        #Freeze bert layers\n",
        "        if freeze_bert:\n",
        "            for p in self.bert_layer.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        #Classification layer\n",
        "        self.dropout = nn.Dropout(p=0.2, inplace=False)\n",
        "        self.cls_layer = nn.Linear(768, 1)\n",
        "\n",
        "    def forward(self, input_ids, attn_masks):\n",
        "        '''\n",
        "        Inputs:\n",
        "            -input_ids : Tensor of shape [batch_size, max_length] containing token ids of sequences\n",
        "            -attn_masks : Tensor of shape [batch_size, max_length] containing attention masks to be used to avoid contibution of PAD tokens\n",
        "        '''\n",
        "\n",
        "        #Feeding the input to BERT model to obtain contextualized representations\n",
        "        if self.bert_type == 'distil-multilingual':\n",
        "          cont_reps = self.bert_layer(input_ids, attention_mask = attn_masks)\n",
        "          cont_reps = cont_reps[0]\n",
        "        else: \n",
        "          cont_reps, _ = self.bert_layer(input_ids, attention_mask = attn_masks)\n",
        "\n",
        "        #Obtaining the representation of [CLS] head\n",
        "        cls_rep = cont_reps[:, 0]\n",
        "\n",
        "        #Feeding cls_rep to the classifier layer\n",
        "        dropout = self.dropout(cls_rep)\n",
        "        logits = self.cls_layer(dropout)\n",
        "        logits = logits.squeeze(-1)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJiv-rt5P_cv",
        "colab_type": "text"
      },
      "source": [
        "Let's define train and evaluation steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6jKPpKjQBbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_accuracy_from_logits(logits, labels):\n",
        "    probs = torch.sigmoid(logits)\n",
        "    soft_probs = (probs > 0.5).long()\n",
        "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
        "    return acc\n",
        "\n",
        "def evaluate(net, criterion, dataloader):\n",
        "    net.eval()\n",
        "\n",
        "    mean_acc, mean_loss = 0, 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attn_masks, labels in dataloader:\n",
        "            input_ids, attn_masks, labels = input_ids.cuda(), attn_masks.cuda(), labels.cuda()\n",
        "            logits = net(input_ids, attn_masks)\n",
        "            mean_loss += criterion(logits, labels).item()\n",
        "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
        "            count += 1\n",
        "\n",
        "    return mean_acc / count, mean_loss / count\n",
        "\n",
        "def train(net, criterion, opti, train_loader, val_loader):\n",
        "    step = 0\n",
        "    best_acc = 0\n",
        "    for ep in range(MAX_EPS):\n",
        "        net.train()\n",
        "        for it, (input_ids, attn_masks, labels) in enumerate(train_loader):\n",
        "            step +=1\n",
        "\n",
        "            #Clear gradients\n",
        "            # Since the backward() function accumulates gradients, and you do not want to mix up \n",
        "            # gradients between different batches, you have to zero them out at the start of a new batch.\n",
        "            opti.zero_grad()\n",
        "            #Converting these to cuda tensors\n",
        "            input_ids, attn_masks, labels = input_ids.cuda(), attn_masks.cuda(), labels.cuda()\n",
        "\n",
        "            #Obtaining the logits from the model\n",
        "            logits = net(input_ids, attn_masks)\n",
        "\n",
        "            #Computing loss\n",
        "            loss = criterion(logits, labels)\n",
        "            # For multiclass case, when criterion = nn.CrossEntropyLoss()\n",
        "            # loss = loss = criterion(logits.squeeze(-1), labels.long())\n",
        "\n",
        "\n",
        "            #Backpropagating the gradients\n",
        "            loss.backward()\n",
        "\n",
        "            #Optimization step\n",
        "            opti.step()\n",
        "\n",
        "            if step % 100 == 0:\n",
        "                acc = get_accuracy_from_logits(logits, labels)\n",
        "                print(\"Iteration {} of epoch {} complete. Loss : {} Accuracy : {}\".format(step, ep, loss.item(), acc))\n",
        "                steps.append(step)\n",
        "                loss_train.append(loss.item())\n",
        "\n",
        "\n",
        "        val_acc, val_loss = evaluate(net, criterion, val_loader)\n",
        "        print(\"Epoch {} complete! Validation Accuracy : {}, Validation Loss : {}\".format(ep, val_acc, val_loss))\n",
        "        steps_valid.append(step)\n",
        "        loss_valid.append(val_loss)\n",
        "        if val_acc > best_acc:\n",
        "            print(\"Best validation accuracy improved from {} to {}, saving model...\".format(best_acc, val_acc))\n",
        "            best_acc = val_acc\n",
        "            torch.save(net.state_dict(), 'Models/{}_fine-tuned_{}.pt'.format(bert_type, ep))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU_UaialFQ_K",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEZw9c9gQS6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 172\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "\n",
        "net = Net(freeze_bert = False, bert_type = 'ita-gilberto')\n",
        "\n",
        "#Enable gpu support for the model\n",
        "net.cuda() \n",
        "\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "opti = torch.optim.Adam(net.parameters(), lr = 0.000005)\n",
        "\n",
        "\n",
        "steps, steps_valid = [], []\n",
        "loss_train, loss_valid = [], []\n",
        "train(net, criterion, opti, train_loader, val_loader)\n",
        "\n",
        "print(loss_train)\n",
        "plt.figure(figsize=(15, 12))\n",
        "plt.plot(steps, loss_train, c = 'b')\n",
        "plt.plot(steps_valid, loss_valid, c = 'g')\n",
        "plt.savefig('./foo.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcRKOI718QPo",
        "colab_type": "text"
      },
      "source": [
        "Save the model on your drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i-f-e3ZRctH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount Google Drive to this Notebook instance.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRRbh249RfGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r ./Models/ ./drive/My\\ Drive/tmp_stuff/BERTCheckpoints/Models/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O48lRrc97WNz",
        "colab_type": "text"
      },
      "source": [
        "## Model Training (with Pytorch Lightning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzCwcKK_MiPS",
        "colab_type": "text"
      },
      "source": [
        "PyTorch Lightning is nothing more than organized PyTorch code. Once you haveve organized it into a LightningModule, it automates most of the training for you. For instange, you get GPU support without writing any of that code in your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJChL1q2FwvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install -q pytorch-lightning"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3Wt3fkQFUor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from torch.nn import functional as F\n",
        "import pytorch_lightning as pl\n",
        "from argparse import Namespace\n",
        "from torch.nn import functional as F\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK-1ATgxcWgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SSTDataset(Dataset):\n",
        "\n",
        "    def __init__(self, filename, maxlen, bert_type = 'ita-gilberto'):\n",
        "\n",
        "        #Store the contents of the file in a pandas dataframe\n",
        "        self.df = pd.read_csv(filename, delimiter = '\\t', header=None)\n",
        "        self.maxlen = maxlen\n",
        "\n",
        "        #Initialize the BERT tokenizer\n",
        "        if bert_type == 'ita-gilberto':\n",
        "            # Italian BERT (GilBERTo)\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\"idb-ita/gilberto-uncased-from-camembert\", do_lower_case=True)\n",
        "        elif bert_type == 'ita-umberto':\n",
        "            # Italian BERT (UmBERTo)\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\"Musixmatch/umberto-commoncrawl-cased-v1\", do_lower_case=True)\n",
        "        elif bert_type == 'multilingual':\n",
        "            # Multilingual\n",
        "            self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True)\n",
        "        elif bert_type == 'distil-multilingual': \n",
        "            # Distilled Multilingual\n",
        "            self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased', do_lower_case=True)\n",
        "\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        #Selecting the sentence and label at the specified index in the data frame\n",
        "        sentence = self.df.loc[index, 0]\n",
        "        label = self.df.loc[index, 1]\n",
        "\n",
        "        # tokenize the sentence\n",
        "        try:\n",
        "            encoded_sent = self.tokenizer.encode(sentence, add_special_tokens = True)\n",
        "        except:\n",
        "            print(\"---- CHECK THIS SENTENCE ----- >>\", sentence, label)\n",
        "            sys.exit()\n",
        "\n",
        "        # padding the sentence to the max length\n",
        "        if len(encoded_sent) > self.maxlen:\n",
        "            encoded_sent = encoded_sent[:self.maxlen - 2]\n",
        "        padded_sent  = encoded_sent + [0 for _ in range(self.maxlen - len(encoded_sent))]\n",
        "        #Converting the list to a pytorch tensor\n",
        "        tokens_ids_tensor = torch.tensor(padded_sent) #Converting the list to a pytorch tensor\n",
        "        #Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
        "        attn_mask = (tokens_ids_tensor != 0).long()\n",
        "\n",
        "        label = torch.tensor(label, dtype=torch.float)\n",
        "\n",
        "        return (tokens_ids_tensor, attn_mask), label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efCB95OkccKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAXLEN= 50\n",
        "bert_type = 'ita-gilberto' #'ita-umberto'\n",
        "BATCH_SIZE = 8\n",
        "MAX_EPS = 5\n",
        "\n",
        "#Creating dataloaders\n",
        "train_set = SSTDataset(filename = './train_sei_short.tsv', maxlen = MAXLEN, bert_type = bert_type, pyt_lightning = True)\n",
        "val_set = SSTDataset(filename = './valid_sei_short.tsv', maxlen = MAXLEN, bert_type = bert_type, pyt_lightning = True)\n",
        "test_set = SSTDataset(filename = './test_sei_short.tsv', maxlen = MAXLEN, bert_type = bert_type, pyt_lightning = True)\n",
        "\n",
        "# Data loader combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
        "train_loader = DataLoader(train_set, batch_size = BATCH_SIZE , num_workers = 2)\n",
        "val_loader = DataLoader(val_set, batch_size = BATCH_SIZE , num_workers = 2)\n",
        "test_loader = DataLoader(test_set, batch_size = BATCH_SIZE , num_workers = 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIWhnWzq7p70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(pl.LightningModule):\n",
        "    # -------------------------\n",
        "    # Italian pre-trained Model with a binary classification head.\n",
        "    # -------------------------\n",
        "    def __init__(self, freeze_bert = True, bert_type = 'ita-gilberto'):\n",
        "        super(Net, self).__init__()\n",
        "        #Instantiating BERT model object\n",
        "        self.bert_type = bert_type \n",
        "        if bert_type == 'ita-gilberto':\n",
        "            # Italian BERT (GilBERTo)\n",
        "            self.bert_layer = AutoModel.from_pretrained(\"idb-ita/gilberto-uncased-from-camembert\")\n",
        "        elif bert_type == 'ita-umberto':\n",
        "            # Italian BERT (UmBERTo)\n",
        "            self.bert_layer = AutoModel.from_pretrained(\"Musixmatch/umberto-commoncrawl-cased-v1\")\n",
        "        elif bert_type == 'multilingual':\n",
        "            # Multilingual\n",
        "            self.bert_layer = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "        elif bert_type == 'distil-multilingual': \n",
        "          # Distilled Multilingual\n",
        "          self.bert_layer = DistilBertModel.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
        "\n",
        "        #Freeze bert layers\n",
        "        if freeze_bert:\n",
        "            for p in self.bert_layer.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        #Classification layer\n",
        "        self.dropout = nn.Dropout(p=0.2, inplace=False)\n",
        "        self.cls_layer = nn.Linear(768, 1)\n",
        "\n",
        "    #def forward(self, input_ids, attn_masks ):\n",
        "    def forward(self, inputs_, *args ):\n",
        "        #print(inputs_)\n",
        "        input_ids, attn_masks = inputs_\n",
        "\n",
        "        '''\n",
        "        Inputs:\n",
        "            -input_ids : Tensor of shape [batch_size, max_length] containing token ids of sequences\n",
        "            -attn_masks : Tensor of shape [batch_size, max_length] containing attention masks to be used to avoid contibution of PAD tokens\n",
        "        '''\n",
        "        #Feeding the input to BERT model to obtain contextualized representations\n",
        "        if self.bert_type == 'distil-multilingual':\n",
        "          cont_reps = self.bert_layer(input_ids, attention_mask = attn_masks)\n",
        "          cont_reps = cont_reps[0]\n",
        "        else: \n",
        "          cont_reps, _ = self.bert_layer(input_ids, attention_mask = attn_masks)\n",
        "\n",
        "        #Obtaining the representation of [CLS] head\n",
        "        cls_rep = cont_reps[:, 0]\n",
        "\n",
        "        #Feeding cls_rep to the classifier layer\n",
        "        dropout = self.dropout(cls_rep)\n",
        "        logits = self.cls_layer(dropout)\n",
        "        logits = logits.squeeze(-1)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8-IE6aOHLS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FineTuningModule(pl.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__()\n",
        "        self.model = Net(freeze_bert = False, bert_type = 'ita-gilberto')\n",
        "        self.hparams = hparams\n",
        "\n",
        "    def forward(self, X, *args):\n",
        "        return self.model(X, *args)\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        # batch: is the output of your dataloader\n",
        "        input_ids_and_attn_masks_touple, labels = batch\n",
        "         \n",
        "        # fwd\n",
        "        logits = self.forward(input_ids_and_attn_masks_touple)\n",
        "\n",
        "        # loss\n",
        "        loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
        "\n",
        "        # logs\n",
        "        tensorboard_logs = {'train_loss': loss}\n",
        "        return {'loss': loss, 'log': tensorboard_logs}\n",
        "\n",
        "    def validation_step(self, batch, batch_nb):\n",
        "        # batch: is the output of your dataloader\n",
        "        input_ids_and_attn_masks_touple, labels = batch\n",
        "         \n",
        "        # fwd\n",
        "        logits = self.forward(input_ids_and_attn_masks_touple)\n",
        "        \n",
        "        # loss\n",
        "        loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
        "\n",
        "        # transform logits in probabilities\n",
        "        probs = torch.sigmoid(logits.squeeze(-1))\n",
        "        # transform probabilities in 0 (probs < .5) or 1 (prob > .5)\n",
        "        soft_probs = (probs > 0.5).long()\n",
        "\n",
        "        # compute accuracy\n",
        "        val_acc = accuracy_score(soft_probs.cpu(), labels.cpu())\n",
        "        val_acc = torch.tensor(val_acc)\n",
        "\n",
        "        #tensorboard_logs = {'val_loss': loss}\n",
        "        return {'val_loss': loss, 'val_acc': val_acc}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
        "        avg_val_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
        "\n",
        "        tensorboard_logs = {'val_loss': avg_loss, 'avg_val_acc': avg_val_acc}\n",
        "        return {'val_loss': avg_loss, 'progress_bar': tensorboard_logs, 'log': tensorboard_logs}\n",
        "\n",
        "    def test_step(self, batch, batch_nb):\n",
        "        input_ids_and_attn_masks_touple, labels  = batch\n",
        "        \n",
        "        logits = self.forward(input_ids_and_attn_masks_touple)\n",
        "        \n",
        "        # transform logits in probabilities\n",
        "        probs = torch.sigmoid(logits.squeeze(-1))\n",
        "        # transform probabilities in 0 (probs < .5) or 1 (prob > .5)\n",
        "        soft_probs = (probs > 0.5).long()\n",
        "\n",
        "        # compute accuracy\n",
        "        test_acc = accuracy_score(soft_probs.cpu(), labels.cpu())\n",
        "        test_acc = torch.tensor(test_acc)\n",
        "        \n",
        "        return {'test_acc': torch.tensor(test_acc)}\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "\n",
        "        avg_test_acc = torch.stack([x['test_acc'] for x in outputs]).mean()\n",
        "\n",
        "        tensorboard_logs = {'avg_test_acc': avg_test_acc}\n",
        "        return {'avg_test_acc': avg_test_acc, 'log': tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=self.hparams.lr, eps=1e-08)\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return train_loader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return val_loader\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return test_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xBE_EbYt4OB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torch-lr-finder==0.1.5\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdpqTYOmh8XE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch_lr_finder import LRFinder\n",
        "\n",
        "lr=0.01\n",
        "hparams_tmp = Namespace(lr=lr)\n",
        "\n",
        "module = FineTuningModule(hparams_tmp)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(module.parameters(), lr=1e-7)\n",
        "lr_finder = LRFinder(module, optimizer, criterion, device=\"cuda\")\n",
        "lr_finder.range_test(module.train_dataloader(), end_lr=100, num_iter=100)\n",
        "lr_finder.plot()# to inspect the loss-learning rate graph\n",
        "lr_finder.reset() # to reset the model and optimizer to their initial state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4YfU9KKvbRA",
        "colab_type": "text"
      },
      "source": [
        "It is recommended to not pick the learning rate that achives the lowest loss, but instead something in the middle of the sharpest downward slope!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCFGqIVzAIvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf lightning_logs/\n",
        "!rm -rf Models/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAWdN9rf5Stt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## garbage collection\n",
        "import gc; gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIdrFjlePImc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "lr=0.00001\n",
        "hparams_tmp = Namespace(lr=lr)\n",
        "finetuner = FineTuningModule(hparams_tmp )\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(filepath='./Models/lightninig_{epoch}-{val_loss:.2f}')\n",
        "#checkpoint_callback = ModelCheckpoint(filepath='./Models/lightning_model')\n",
        "\n",
        "# most basic trainer, uses good defaults (1 gpu)\n",
        "trainer = pl.Trainer(gpus=1, max_epochs=3, checkpoint_callback=checkpoint_callback)    \n",
        "trainer.fit(finetuner) \n",
        "\n",
        "# train on 1024 CPUs across 128 machines\n",
        "# trainer = pl.Trainer(\n",
        "#     num_processes=8,\n",
        "#     num_nodes=128 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqYWEVyw_Wfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start tensorboard.\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2umTImZUq8jr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ftnet =  FineTuningModule.load_from_checkpoint('./Models/lightninig_epoch=2-val_loss=0.04.ckpt')\n",
        "trainer = pl.Trainer(gpus=1)\n",
        "trainer.test(ftnet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4g7bGYB7Hrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ftnet.freeze()\n",
        "\n",
        "encoded_sent = tokenizer.encode('ci sei dopo in piazza duomo', add_special_tokens = True)\n",
        "tokens_from_ids = tokenizer.convert_ids_to_tokens(encoded_sent)\n",
        "\n",
        "padded_sent  = encoded_sent + [0 for _ in range(50 - len(encoded_sent))]\n",
        "attention_masks = [float(i>0) for i in padded_sent]\n",
        "\n",
        "inputs = torch.tensor([padded_sent])\n",
        "inputs = inputs.cuda()\n",
        "masks = torch.tensor([attention_masks])\n",
        "masks = masks.cuda()\n",
        "\n",
        "\n",
        "# or for prediction\n",
        "logits = ftnet(inputs, masks)\n",
        "\n",
        "probs = torch.sigmoid(logits.squeeze(-1))\n",
        "\n",
        "probs.cpu()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtTvmUEra2SK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## garbage collection\n",
        "import gc; gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLcn50uqPp_n",
        "colab_type": "text"
      },
      "source": [
        "## Before Inference: Model Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFrvd16R7UhN",
        "colab_type": "text"
      },
      "source": [
        "The task is the disambiguation of the word `sei` which has at least two meanings: verb and number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ7BQX9ZoSN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Download Checkpoint already Trained\n",
        "\n",
        "# Italian pre-trained Gilberto Model with a binary classification head, finetuned on SEI disambiguation.\n",
        "!gdown --id 18YMFrh3CIWzcL7HAPLQkaXKR7HPBVhgd\n",
        "\n",
        "# Italian pre-trained Umberto Model with a binary classification head, finetuned on SEI disambiguation.\n",
        "!gdown --id 1-44hscILUBFMHseoflGyBt9w2QWYTkvp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DD2Z_6wWHBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Move checkpoint to Models\n",
        "!mkdir Models\n",
        "!mv *_sei.pt ./Models/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eDPxAsaL6WV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#bert_type = 'ita-umberto'\n",
        "bert_type = 'ita-gilberto'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob9MxVD_YpA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    # -------------------------\n",
        "    # Italian pre-trained Model (No fine-tuning applied)\n",
        "    # -------------------------\n",
        "    def __init__(self, freeze_bert = False, bert_type = 'ita-gilberto'):\n",
        "        super(Net, self).__init__()\n",
        "        #Instantiating BERT model object\n",
        "        if bert_type == 'ita-gilberto':\n",
        "            # Italian BERT (GilBERTo)\n",
        "            self.bert_layer = AutoModel.from_pretrained(\"idb-ita/gilberto-uncased-from-camembert\")\n",
        "        elif bert_type == 'ita-umberto':\n",
        "            # Italian BERT (UmBERTo)\n",
        "            self.bert_layer = AutoModel.from_pretrained(\"Musixmatch/umberto-commoncrawl-cased-v1\")\n",
        "        elif bert_type == 'multilingual':\n",
        "            # Multilingual\n",
        "            self.bert_layer = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "        elif bert_type == 'distil-multilingual': \n",
        "          # Distilled Multilingual\n",
        "          self.bert_layer = DistilBertModel.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
        "\n",
        "        #Freeze bert layers\n",
        "        if freeze_bert:\n",
        "            for p in self.bert_layer.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "\n",
        "    def forward(self, seq, attn_masks):\n",
        "        #Feeding the input to BERT model to obtain contextualized representations\n",
        "        cont_reps, _ = self.bert_layer(seq, attention_mask = attn_masks)\n",
        "\n",
        "        return _, cont_reps\n",
        "\n",
        "\n",
        "class FineTunedNet(nn.Module):\n",
        "      # -------------------------\n",
        "      # Italian pre-trained Model with a binary classification head.\n",
        "      # -------------------------\n",
        "    def __init__(self, freeze_bert = False, bert_type = 'ita-gilberto'):\n",
        "        super(FineTunedNet, self).__init__()\n",
        "        #Instantiating BERT model object\n",
        "        self.bert_type = bert_type \n",
        "        if bert_type == 'ita-gilberto':\n",
        "            # Italian BERT (GilBERTo)\n",
        "            self.bert_layer = AutoModel.from_pretrained(\"idb-ita/gilberto-uncased-from-camembert\")\n",
        "        elif bert_type == 'ita-umberto':\n",
        "            # Italian BERT (UmBERTo)\n",
        "            self.bert_layer = AutoModel.from_pretrained(\"Musixmatch/umberto-commoncrawl-cased-v1\")\n",
        "        elif bert_type == 'multilingual':\n",
        "            # Multilingual\n",
        "            self.bert_layer = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "        elif bert_type == 'distil-multilingual': \n",
        "          # Distilled Multilingual\n",
        "          self.bert_layer = DistilBertModel.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
        "\n",
        "        #Freeze bert layers\n",
        "        if freeze_bert:\n",
        "            for p in self.bert_layer.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        #Classification layer\n",
        "        self.dropout = nn.Dropout(p=0.2, inplace=False)\n",
        "        self.cls_layer = nn.Linear(768, 1)\n",
        "\n",
        "    def forward(self, seq, attn_masks):\n",
        "\n",
        "        #Feeding the input to BERT model to obtain contextualized representations\n",
        "        if self.bert_type == 'distil-multilingual':\n",
        "          cont_reps = self.bert_layer(seq, attention_mask = attn_masks)\n",
        "          cont_reps = cont_reps[0]\n",
        "        else: \n",
        "          cont_reps, _ = self.bert_layer(seq, attention_mask = attn_masks)\n",
        "\n",
        "        #Obtaining the representation of [CLS] head\n",
        "        cls_rep = cont_reps[:, 0]\n",
        "\n",
        "        #Feeding cls_rep to the classifier layer\n",
        "        dropout = self.dropout(cls_rep)\n",
        "        logits = self.cls_layer(dropout)\n",
        "\n",
        "        return logits, cont_reps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efQVUOAy-LOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the pre-trained model not finetuned (for visualization purposes in the last section)\n",
        "\n",
        "net = Net(bert_type = bert_type)\n",
        "net.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tXx4nrVPjzZ7",
        "colab": {}
      },
      "source": [
        "#Load the model with the checkpoint (pre-trained+classification layer. All finetuned on our dataset \"sei vs 6\")\n",
        "\n",
        "fineTunedNet = FineTunedNet(bert_type = bert_type)\n",
        "\n",
        "if bert_type == 'ita-gilberto':\n",
        "  fineTunedNet.load_state_dict(torch.load('./Models/gilberto_finetuned_sei.pt'))\n",
        "elif bert_type == 'ita-umberto':\n",
        "  fineTunedNet.load_state_dict(torch.load('./Models/umberto_finetuned_sei.pt'))\n",
        "\n",
        "fineTunedNet.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDi93Fgf6K5z",
        "colab_type": "text"
      },
      "source": [
        "## Inference and Embedding Visualization\n",
        "\n",
        "Below we define a couple of useful functions. The first one, given a sentence, will predict its class. The second one will extract embeddings. If the arg `specific_token = ciao` it will extract the embedding of the token `ciao`. If specific_token = `<s>`, it will extract the classification special token ([CLS] for BERT, \\<s\\> for Roberta). If `specific_token = None`, it will extract an average of all token embeddings (except the special token ones that are removed)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8bRmc8uiIdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_gil = AutoTokenizer.from_pretrained(\"idb-ita/gilberto-uncased-from-camembert\", do_lower_case=True)\n",
        "tokenizer_umb = AutoTokenizer.from_pretrained(\"Musixmatch/umberto-commoncrawl-cased-v1\", do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzl_O8iGptc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN=50\n",
        "model_finetuned = fineTunedNet\n",
        "tokenizer = tokenizer_gil if bert_type == 'ita-gilberto' else tokenizer_umb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKncvO_T4AVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_inference(model, single_sentence, tokenizer, PRINT=False):\n",
        "    # tokenize properly the sentence\n",
        "    encoded_sent = tokenizer.encode(single_sentence, add_special_tokens = True)\n",
        "    tokens_from_ids = tokenizer.convert_ids_to_tokens(encoded_sent)\n",
        "    if PRINT: print(tokens_from_ids)\n",
        "    tokens_length = len(encoded_sent)\n",
        "    if PRINT: print(encoded_sent)\n",
        "    # apply padding\n",
        "    padded_sent  = encoded_sent + [0 for _ in range(MAX_LEN - len(encoded_sent))]\n",
        "    if PRINT: print(padded_sent)\n",
        "    # apply mask\n",
        "    attention_masks = [float(i>0) for i in padded_sent]\n",
        "    if PRINT: print(attention_masks)\n",
        "    # convert to tensors\n",
        "    inputs = torch.tensor([padded_sent])\n",
        "    masks = torch.tensor([attention_masks])\n",
        "    # BertModel\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      logits, _ = model(inputs, masks)\n",
        "    \n",
        "    #logits = logits.detach().cpu()\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "    return probs\n",
        "\n",
        "def embedding_extractor(model, single_sentence, tokenizer, specific_token=None, PRINT=True):\n",
        "    # tokenize properly the sentence\n",
        "    encoded_sent = tokenizer.encode(single_sentence, add_special_tokens = True)\n",
        "    tokens_from_ids = tokenizer.convert_ids_to_tokens(encoded_sent)\n",
        "    if PRINT: print(tokens_from_ids)\n",
        "    tokens_length = len(encoded_sent)\n",
        "    if PRINT: print(encoded_sent)\n",
        "    # apply padding\n",
        "    padded_sent  = encoded_sent + [0 for _ in range(MAX_LEN - len(encoded_sent))]\n",
        "    if PRINT: print(padded_sent)\n",
        "    # apply mask\n",
        "    attention_masks = [float(i>0) for i in padded_sent]\n",
        "    if PRINT: print(attention_masks)\n",
        "    # convert to tensors\n",
        "    inputs = torch.tensor([padded_sent])\n",
        "    masks = torch.tensor([attention_masks])\n",
        "    # BertModel\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "      _, last_hidden_states  = model(inputs, masks)\n",
        "    # ---------- all tokens extractor -------------\n",
        "    last_hidden_states_tokens = last_hidden_states[0][0:tokens_length]\n",
        "    #token_and_emb = zip(tokens_from_ids,last_hidden_states_tokens)\n",
        "    # ---------- specific token extractor -------------\n",
        "    if specific_token is not None:\n",
        "      tokens_cleaned = list(map(lambda item: re.sub(r'▁', '', item), tokens_from_ids))\n",
        "      token_indices = np.where(np.array(tokens_cleaned) == specific_token)[0]\n",
        "      selected_tokens = [tokens_from_ids[i] for i in token_indices]\n",
        "      selected_embeddings = [last_hidden_states_tokens[i] for i in token_indices]\n",
        "      return selected_tokens, selected_embeddings\n",
        "    else:\n",
        "      remove_first_and_last = last_hidden_states_tokens.numpy()[1:-1]\n",
        "      mean_embedding = np.mean(remove_first_and_last, axis=0)\n",
        "      return tokens_from_ids, mean_embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0t8-ezk78t6",
        "colab_type": "text"
      },
      "source": [
        "Let us have a look step-by-step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46nqpvpl7y3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prob = model_inference(model_finetuned, 'arrivato fino a qua, sei ancora attento?', tokenizer, PRINT=True)\n",
        "prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytYi5wLCnX8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token, emb = embedding_extractor(model_finetuned, 'arrivato fino a qua, sei ancora attento?', tokenizer, specific_token='sei', PRINT=True)\n",
        "token, emb[0].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTe3Tp8b-A0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the test data\n",
        "path_to_data = './test_sei.tsv'\n",
        "df_test = pd.read_csv(path_to_data, sep='\\t', header=None)\n",
        "\n",
        "# build up a dataframe\n",
        "df_test['sentence'] = df_test[0]\n",
        "df_test['label'] = df_test[1]\n",
        "df_test = df_test[['sentence','label']]\n",
        "df_test = df_test.dropna()\n",
        "\n",
        "tot_length = len(df_test)\n",
        "print(tot_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAufU-m-5498",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test = df_test.head(200)\n",
        "df_test.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtQdW7-O1fJT",
        "colab_type": "text"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wloNxXgCtV1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_predictions = []\n",
        "for sent in df_test.sentence.values:\n",
        "  pred = model_inference(model_finetuned,  sent , tokenizer, PRINT=False)\n",
        "  pred = 0 if pred < 0.5 else 1\n",
        "  model_predictions.append(pred)\n",
        "\n",
        "model_predictions = np.array(model_predictions)\n",
        "true_labels = df_test.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoINSkNP2jjV",
        "colab_type": "text"
      },
      "source": [
        "Some metrics and confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzNw048jj2Ux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sn\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "def show_results(true_labels, predictions, ONLY_CFS = False):\n",
        "  cfs_mtrx = confusion_matrix(true_labels, predictions)\n",
        "  if not ONLY_CFS:\n",
        "    print(\"Accuracy: %.2f%%\" % (accuracy_score(true_labels, predictions)*100))\n",
        "    print(classification_report(true_labels, predictions))\n",
        "    print(pd.DataFrame(cfs_mtrx))\n",
        "  else:\n",
        "    sn.set(font_scale=1.4) # for label size\n",
        "    sn.heatmap(pd.DataFrame(cfs_mtrx/float(tot_length)), annot=True, annot_kws={\"size\": 16}) # font size\n",
        "    plt.show()\n",
        "\n",
        "show_results(true_labels, model_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y6BdFKUj-HS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_results(true_labels, model_predictions, ONLY_CFS = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anQUWxqWm08V",
        "colab_type": "text"
      },
      "source": [
        "### Embedding Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtQPOoMamuTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_finetuned = fineTunedNet\n",
        "model_non_finetuned = net\n",
        "\n",
        "SEED=172"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dxw4oF71DHV",
        "colab_type": "text"
      },
      "source": [
        "#### Extract Embeddings\n",
        "\n",
        "We first extract some embeddings (CLS token and embedding of 'sei'). Then build a dataframe with all the collected information (UMAP, TSNE ecc)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPiDKANNmzv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "finetuned_embs_cls_token=[]\n",
        "for sentence in df_test['sentence'].values:\n",
        "  _, emb = embedding_extractor(model_finetuned, sentence, tokenizer, specific_token='<s>', PRINT=False)\n",
        "  finetuned_embs_cls_token.append(emb)\n",
        "\n",
        "finetuned_embs_specific_token=[]\n",
        "for sentence in df_test['sentence'].values:\n",
        "  _, emb = embedding_extractor(model_finetuned, sentence, tokenizer, specific_token='sei', PRINT=False)\n",
        "  finetuned_embs_specific_token.append(emb)\n",
        "\n",
        "# For NON finetuned Model\n",
        "embs_specific_token=[]\n",
        "for sentence in df_test['sentence'].values:\n",
        "  _, emb = embedding_extractor(model_non_finetuned, sentence, tokenizer, specific_token='sei', PRINT=False)\n",
        "  embs_specific_token.append(emb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8tzrML4SGAd",
        "colab_type": "text"
      },
      "source": [
        "We use UMAP or TSNE to project 768 dimensions (dim of BERT embeddings) down to 2 dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbjDpgDromkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import umap\n",
        "\n",
        "#--------------------------------------#\n",
        "SEED = 172\n",
        "random.seed(SEED)\n",
        "#--------------------------------------#\n",
        "\n",
        "# UMAP Proj\n",
        "reducer = umap.UMAP(random_state = SEED)\n",
        "# Plotting the average of last layer embeddings\n",
        "umap_2d_token = reducer.fit_transform([np.array(emb[0]) for emb in embs_specific_token])\n",
        "# Plotting the first classification token\n",
        "umap_2d_ft_cls = reducer.fit_transform([np.array(emb[0]) for emb in finetuned_embs_cls_token])\n",
        "# Plotting the a specific token\n",
        "umap_2d_ft_token = reducer.fit_transform([np.array(emb[0]) for emb in finetuned_embs_specific_token])\n",
        "\n",
        "#TSNE Proj\n",
        "tsne = TSNE(n_components=2, random_state = SEED)\n",
        "# Plotting the average of last layer embeddings\n",
        "tsne_2d_token = tsne.fit_transform([np.array(emb[0]) for emb in embs_specific_token])\n",
        "# Plotting the first classification token\n",
        "tsne_2d_ft_cls = tsne.fit_transform([np.array(emb[0]) for emb in finetuned_embs_cls_token])\n",
        "tsne_2d_ft_token = tsne.fit_transform([np.array(emb[0]) for emb in finetuned_embs_specific_token])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7WG8QBzptmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create column with classes as string (for plotting reasons)\n",
        "dict_label_number = {0: 'class_0', 1: 'class_1', 2: 'class_2'}\n",
        "\n",
        "df_test['type'] = list(map(lambda x: dict_label_number[x], df_test['label'].values))\n",
        "df_test['label'].astype(int)\n",
        "\n",
        "# create column with umap 2d-coordinates for the pretrained gilberto model\n",
        "df_test['umap_2d_0_token'] = umap_2d_token[:,0]\n",
        "df_test['umap_2d_1_token'] = umap_2d_token[:,1]\n",
        "\n",
        "#create column with umap 2d-coordinates for the fine-tuned classification token of gilberto model\n",
        "df_test['umap_2d_0_ft_cls'] = umap_2d_ft_cls[:,0]\n",
        "df_test['umap_2d_1_ft_cls'] = umap_2d_ft_cls[:,1]\n",
        "\n",
        "\n",
        "#create column with umap 2d-coordinates for the fine-tuned specific token of gilberto model\n",
        "df_test['umap_2d_0_ft_token'] = umap_2d_ft_token[:,0]\n",
        "df_test['umap_2d_1_ft_token'] = umap_2d_ft_token[:,1]\n",
        "\n",
        "# create column with tsne 2d-coordinates for the pretrained gilberto model\n",
        "df_test['tsne_2d_0_token'] = tsne_2d_token[:,0]\n",
        "df_test['tsne_2d_1_token'] = tsne_2d_token[:,1]\n",
        "\n",
        "# create column with tsne 2d-coordinates for the fine-tuned classification token of gilberto model\n",
        "df_test['tsne_2d_0_ft_cls'] = tsne_2d_ft_cls[:,0]\n",
        "df_test['tsne_2d_1_ft_cls'] = tsne_2d_ft_cls[:,1]\n",
        "\n",
        "#create column with tsne 2d-coordinates for the fine-tuned specific token of gilberto model\n",
        "df_test['tsne_2d_0_ft_token'] = tsne_2d_ft_token[:,0]\n",
        "df_test['tsne_2d_1_ft_token'] = tsne_2d_ft_token[:,1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt_di7UArV0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.to_csv('./df_sei.tsv', header=False, index=False, sep='\\t')\n",
        "df_test.head(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSPUc6Rt1b1D",
        "colab_type": "text"
      },
      "source": [
        "#### Embedding Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX0m_O6YteBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Choose what to visualize: UMAP, TSNE, BOTH\n",
        "show_proj = 'BOTH'\n",
        "\n",
        "# Choose if visualize CLS (alias <\\s\\>) token embedding or the embedding of \"sei\"\n",
        "plot_token_embs = True\n",
        "plot_cls_embs = not plot_token_embs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NACHQIKrnPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bokeh.plotting import figure, show\n",
        "from bokeh.layouts import row, column\n",
        "from bokeh.models import HoverTool, ColumnDataSource \n",
        "from bokeh.transform import factor_cmap, factor_mark\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "# Call once to configure Bokeh to display plots inline in the notebook.\n",
        "output_notebook()\n",
        "\n",
        "# to use pandas dataframe\n",
        "source = ColumnDataSource.from_df(df_test) \n",
        "\n",
        "# to use some tools form bokeh\n",
        "tools_to_show = 'box_zoom, save, reset'\n",
        "\n",
        "# colors and markers for each class\n",
        "CLASSES= ['class_0', 'class_1']\n",
        "MARKERS = ['hex', 'triangle']\n",
        "COLORS = [\"#7fc97f\", \"#f0027f\"]\n",
        "\n",
        "if show_proj == 'UMAP':\n",
        "  width, height = 700, 570\n",
        "elif show_proj == 'TSNE':\n",
        "  width, height = 700, 570\n",
        "else:\n",
        "  width, height = 600, 400\n",
        "\n",
        "\n",
        "# plot title\n",
        "fig1 = figure(title = \"UMAP 2d - proj - Base\", tools=tools_to_show, plot_width=width, plot_height=height)\n",
        "fig2 = figure(title = \"UMAP 2d - proj - Fine-Tuned Model\", tools=tools_to_show, plot_width=width, plot_height=height)\n",
        "\n",
        "# define columns to plot\n",
        "x1, y1 = \"umap_2d_0_token\", \"umap_2d_1_token\"\n",
        "if plot_cls_embs:\n",
        "  x2, y2 = \"umap_2d_0_ft_cls\", \"umap_2d_1_ft_cls\"\n",
        "elif plot_token_embs:\n",
        "  x2, y2 = \"umap_2d_0_ft_token\", \"umap_2d_1_ft_token\"\n",
        "\n",
        "\n",
        "fig1.xaxis.axis_label, fig1.yaxis.axis_label = x1, y1\n",
        "fig2.xaxis.axis_label, fig2.yaxis.axis_label = x2, y2\n",
        "\n",
        "# hovering tool\n",
        "fig1.add_tools(HoverTool(tooltips = [(\"columan_name\", \"@{sentence}\"),])) \n",
        "fig2.add_tools(HoverTool(tooltips = [(\"columan_name\", \"@{sentence}\"),])) \n",
        "\n",
        "\n",
        "factor_cmap_col = 'type'\n",
        "\n",
        "fig1.scatter(x1, y1, source=source, legend_field=\"label\", fill_alpha=0.4, size=12,\n",
        "          color=factor_cmap(factor_cmap_col, COLORS, CLASSES))\n",
        "fig2.scatter(x2, y2, source=source, legend_field=\"label\", fill_alpha=0.4, size=12,\n",
        "          marker=\"triangle\", color=factor_cmap(factor_cmap_col, COLORS, CLASSES))\n",
        "\n",
        "\n",
        "# plot title\n",
        "fig3 = figure(title = \"TSNE 2d - proj - Base\", tools=tools_to_show, plot_width=width, plot_height=height)\n",
        "fig4 = figure(title = \"TSNE 2d - proj - Fine-Tuned Model\", tools=tools_to_show, plot_width=width, plot_height=height)\n",
        "\n",
        "# define columns to plot\n",
        "x1, y1 = \"tsne_2d_0_token\", \"tsne_2d_1_token\"\n",
        "if plot_cls_embs:\n",
        "  x2, y2 = \"tsne_2d_0_ft_cls\", \"tsne_2d_1_ft_cls\"\n",
        "elif plot_token_embs:\n",
        "  x2, y2 = \"tsne_2d_0_ft_token\", \"tsne_2d_1_ft_token\"  \n",
        "\n",
        "    \n",
        "fig3.xaxis.axis_label, fig1.yaxis.axis_label = x1, y1\n",
        "fig4.xaxis.axis_label, fig2.yaxis.axis_label = x2, y2\n",
        "\n",
        "# hovering tool\n",
        "fig3.add_tools(HoverTool(tooltips = [(\"columan_name\", \"@{sentence}\"),])) \n",
        "fig4.add_tools(HoverTool(tooltips = [(\"columan_name\", \"@{sentence}\"),])) \n",
        "\n",
        "factor_cmap_col = 'type'\n",
        "\n",
        "fig3.scatter(x1, y1, source=source, legend_field=\"label\", fill_alpha=0.4, size=12,\n",
        "          color=factor_cmap(factor_cmap_col, COLORS, CLASSES))\n",
        "\n",
        "fig4.scatter(x2, y2, source=source, legend_field=\"label\", fill_alpha=0.4, size=12,\n",
        "          marker=\"triangle\", color=factor_cmap(factor_cmap_col, COLORS, CLASSES))\n",
        "\n",
        "\n",
        "if show_proj == 'UMAP':\n",
        "  show(row(fig1,fig2))\n",
        "elif show_proj == 'TSNE':\n",
        "  show(row(fig3,fig4))\n",
        "else:\n",
        "  show(column(row(fig1,fig2), row(fig3,fig4)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfrDsdHCj5zz",
        "colab_type": "text"
      },
      "source": [
        "We can easily see that both models (base and finetuned) are able to distinguish the different meanings of the word `sei`. The finetuned model clearly distinguish the two main meaning, being that it was trained to a binary classification task. The base model it is a bit more blurry but this is fine and we can see it is able to distinguish sub clusters of meanings on top of the two main ones (verb and number).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfiAkxDXIjPv",
        "colab_type": "text"
      },
      "source": [
        "## Base Model plus K-means for non-labelled data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR5K0zdOW5H6",
        "colab_type": "text"
      },
      "source": [
        "We've just seen that to distinguish word meanings the pre-trained (and not finetuned) model already give us pretty good results.\n",
        "\n",
        "So, suppose now that we collect some sentences (not labelled) that contain the word `secondo` (it has a lot of meanings) and somehow we would like to separate them according to the different meanings of the word.\n",
        "\n",
        "<center>  <img src=\"https://drive.google.com/uc?id=1ZHPpPGdDxFR0CkUBo_v0NB67gbPDvaNJ\" width=\"650\" height=\"400\">  </center> \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuTLZ3P_S6Jb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gdown --id 1-6IeIpZo3meXDm-_d8zbgZAmy6uW99eF"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qigeSV1ER2NM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = open('./poly_secondo.txt', 'r')\n",
        "sentences = [line.strip() for line in sentences.readlines() if 'secondo' in line.split()]\n",
        "\n",
        "# build up a dataframe\n",
        "df_test = pd.DataFrame(sentences,columns =['sentence']) \n",
        "df_test['sentence'] = sentences\n",
        "df_test = df_test.dropna()\n",
        "\n",
        "df_test = df_test.head(500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwtkg7nGTUFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head -n 10 poly_secondo.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77-JGVi-V3Vo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(df_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq3iPDscTOwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_non_finetuned = net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBHUddsIN9Wl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embs_specific_token=[]\n",
        "for sentence in df_test['sentence'].values:\n",
        "  _, emb = embedding_extractor(model_non_finetuned, sentence, tokenizer, specific_token='secondo', PRINT=False)\n",
        "  embs_specific_token.append(emb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8MvFDX-bopr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import umap\n",
        "\n",
        "#--------------------------------------#\n",
        "SEED = 172\n",
        "random.seed(SEED)\n",
        "#--------------------------------------#\n",
        "\n",
        "# UMAP Proj\n",
        "reducer = umap.UMAP(random_state = SEED)\n",
        "umap_2d_token = reducer.fit_transform([np.array(emb[0]) for emb in embs_specific_token])\n",
        "\n",
        "#TSNE Proj\n",
        "tsne = TSNE(n_components=2, random_state = SEED)\n",
        "tsne_2d_token = tsne.fit_transform([np.array(emb[0]) for emb in embs_specific_token])\n",
        "\n",
        "\n",
        "#create column with umap 2d-coordinates for the fine-tuned specific token of gilberto model\n",
        "df_test['umap_2d_0_token'] = umap_2d_token[:,0]\n",
        "df_test['umap_2d_1_token'] = umap_2d_token[:,1]\n",
        "df_test['tsne_2d_0_token'] = tsne_2d_token[:,0]\n",
        "df_test['tsne_2d_1_token'] = tsne_2d_token[:,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIUmARXwJmE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans_clustering = KMeans( n_clusters = 6 , random_state = SEED)\n",
        "\n",
        "#idx_embs = kmeans_clustering.fit_predict([np.array(emb[0]) for emb in embs])\n",
        "idx_umap_2d  = kmeans_clustering.fit_predict(umap_2d_token)\n",
        "\n",
        "idx_tsne_2d = kmeans_clustering.fit_predict(tsne_2d_token)\n",
        "\n",
        "df_test['idx_umap_2d'] = idx_umap_2d\n",
        "df_test['idx_tsne_2d'] = idx_tsne_2d\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6nMTZesUAGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dict_label_number = {0: 'class_0', 1: 'class_1', 2: 'class_2', 3: 'class_3',  4: 'class_4', 5: 'class_5'}\n",
        "\n",
        "df_test['idx_umap_2d_str'] = list(map(lambda x: dict_label_number[x], df_test['idx_umap_2d'].values))\n",
        "df_test['idx_tsne_2d_str'] = list(map(lambda x: dict_label_number[x], df_test['idx_tsne_2d'].values))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhMFRl_jcSES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bokeh.plotting import figure, show\n",
        "from bokeh.layouts import row, column\n",
        "from bokeh.models import HoverTool, ColumnDataSource \n",
        "from bokeh.transform import factor_cmap, factor_mark\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "# Call once to configure Bokeh to display plots inline in the notebook.\n",
        "output_notebook()\n",
        "\n",
        "# to use pandas dataframe\n",
        "source = ColumnDataSource.from_df(df_test) \n",
        "\n",
        "# to use some tools form bokeh\n",
        "tools_to_show = 'box_zoom, save, reset'\n",
        "\n",
        "# colors and markers for each class\n",
        "CLASSES= ['class_0', 'class_1', 'class_2', 'class_3', 'class_4', 'class_5']\n",
        "#MARKERS = ['hex', 'triangle', 'Cross']\n",
        "COLORS = [\"#7fc97f\", \"#f0027f\", \"#ff0000\", \"#000000\", \"#0000FF\", \"#666633\"]\n",
        "\n",
        "\n",
        "width, height = 1100, 700\n",
        "\n",
        "\n",
        "# plot title\n",
        "fig1 = figure(title = \"UMAP 2d - proj - Gilberto\", tools=tools_to_show, plot_width=width, plot_height=height)\n",
        "fig2 = figure(title = \"TSNE 2d - proj - Gilberto\", tools=tools_to_show, plot_width=width, plot_height=height)\n",
        "\n",
        "# define columns to plot\n",
        "#x0, y0 = \"umap_2d_0\", \"umap_2d_1\"\n",
        "x1, y1 = \"umap_2d_0_token\", \"umap_2d_1_token\"\n",
        "x2, y2 = \"tsne_2d_0_token\", \"tsne_2d_1_token\"\n",
        "\n",
        "\n",
        "fig1.xaxis.axis_label, fig1.yaxis.axis_label = x1, y1\n",
        "fig2.xaxis.axis_label, fig2.yaxis.axis_label = x2, y2\n",
        "\n",
        "# hovering tool\n",
        "fig1.add_tools(HoverTool(tooltips = [(\"columan_name\", \"@{sentence}\"),])) \n",
        "fig2.add_tools(HoverTool(tooltips = [(\"columan_name\", \"@{sentence}\"),])) \n",
        "\n",
        "\n",
        "factor_cmap_col = 'idx_tsne_2d_str'\n",
        "\n",
        "fig1.scatter(x1, y1, source=source, fill_alpha=0.4, size=12,\n",
        "          color=factor_cmap(factor_cmap_col, COLORS, CLASSES))\n",
        "fig2.scatter(x2, y2, source=source, fill_alpha=0.4, size=12,\n",
        "          marker=\"triangle\", color=factor_cmap(factor_cmap_col, COLORS, CLASSES))\n",
        "\n",
        "\n",
        "show(column(fig1,fig2))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}