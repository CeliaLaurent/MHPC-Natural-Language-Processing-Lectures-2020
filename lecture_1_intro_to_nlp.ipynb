{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lecture-1_intro-to-nlp.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PNbGJa5Xkhb1",
        "2n8XiGYekdtn",
        "KWeQN6ugkjir",
        "2lrg8QCvkXmt",
        "ZA_clsOD0NZm",
        "1WyqkEFmFj2O",
        "nSe2tzw5FgaG",
        "USTpjSHcj3bA",
        "K88XqXECTV1f",
        "_nUazbJq1R9Q",
        "pO8d1MqVFZX5"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMRK3Byer+IfH9SZcux+xES",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/denocris/MHPC-Natural-Language-Processing-Lectures-2020/blob/master/lecture_1_intro_to_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvnISh2tMuNU",
        "colab_type": "text"
      },
      "source": [
        "# Introduction Natural Language Processing (Lecture I)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlHISafNWVhC",
        "colab_type": "text"
      },
      "source": [
        "## Presentation and Course Description\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jitys8FWaX8",
        "colab_type": "text"
      },
      "source": [
        "### Who am I?\n",
        "\n",
        "I am Cristiano De Nobili. Here a slide with my path [*from physics to somewhere!*](https://docs.google.com/presentation/d/1xQ5VqDFYvDWu6cmu9OsqnSZGMOV9pgI25Cy2AFprR3k/edit?usp=sharing).\n",
        "\n",
        "\n",
        "### My Contacts\n",
        "For any questions or doubts you can find my contacts here:\n",
        "\n",
        "* [Linkedin](https://www.linkedin.com/in/cristiano-de-nobili/) and [Twitter](https://twitter.com/denocris) (here I regulary post about AI and Science news)\n",
        "* My [Personal Website](https://denocris.com)\n",
        "* My [Instagram](https://www.instagram.com/denocris/?hl=it) (I am a Pilot, so here I mostly post about traveling, flying and adventures)\n",
        "* My recent TEDx on [AI and Human Creativity](https://youtu.be/8-hrmer9d_E)\n",
        "\n",
        "### Course Repository\n",
        "\n",
        "All notebooks can be found [here!](https://github.com/denocris/MHPC-Natural-Language-Processing-Lectures-2020)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZaWqJuZdQSp",
        "colab_type": "text"
      },
      "source": [
        "### Course Presentation and Outline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtSIBL4wkV2r",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "This year's course will be about Deep Learning for Natural Language Processing. It is completely different from last year course. Around 80% of the material is new and state-of-the-art. That means that it did not exist last year. I apologize in advance if there are some typos. I prepared it from scratch just for you. The deep learning framework chosen for this year is [PyTorch](https://pytorch.org/). In addition, we will take advantages of two NLP libraries: [spaCy](https://spacy.io) by Explosion AI and [Transformers](https://github.com/huggingface/transformers) by Hugging Face.\n",
        "\n",
        "If you are interested in last year course, it was about Generative Models. In particular Variational Autoencoders (denoising) and Sequence-to-sequence models (as character-based spell-checker). Here you can find last year course [Repo](https://github.com/denocris/MHPC-DeepLearning-Lectures-2019).\n",
        "\n",
        "#### Outline\n",
        "\n",
        "* Lecture 1: intro to NLP, text preprocessing, spaCy, non-contextual word embedding, SkipGram Word2Vec coded from scratch, pre-trained Glove with Gensim, intro to contextual word embedding.\n",
        "\n",
        "* Lecture 2: NLP with Transformers (by Hugging Face)\n",
        "\n",
        "* Letrue 3: word-sense disambiguation with Italian RoBERTa, embeddings visualization\n",
        "\n",
        "#### Disclaimer\n",
        "\n",
        "* NLP is a huge topics. It is impossible to cover it all in a few hours. If during the lectures some parts of the code appear to you mysterious do not worry. All the notebooks are yours (and it is a lot of material!) and you can play with them in the next weeks if you are interested.\n",
        "\n",
        "* NLP is evolving very fast. For instance, in the last year many fundamental things have changed. From algorithms to tools and libraries. Also for experts of the fields, it is impossible to stay fully updated.\n",
        "\n",
        "\n",
        "#### Additional Refs\n",
        "\n",
        "* [Natural Language Processing with Deep Learning (Stanford Edu)](http://web.stanford.edu/class/cs224n/) \n",
        "* [A Code-First Intro to Natural Language Processing (FastAI)](https://github.com/fastai/course-nlp) \n",
        "\n",
        "To stay updated (first of all follow on twitter and LinkedIn relevant influencers)\n",
        "\n",
        "* [NLP-Progress by Sebastian Ruder (FastAI)](https://nlpprogress.com/) \n",
        "\n",
        "Then a curated list of PyTorch tutorials, projects, libraries\n",
        "\n",
        "* [The incredible PyTorch](https://github.com/ritchieng/the-incredible-pytorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMZasGdRgO3A",
        "colab_type": "text"
      },
      "source": [
        "### Why Colab? \n",
        "Colab (Google Colaboratory) is a free cloud service based on Jupyter Notebooks that supports free GPU!!\n",
        "\n",
        "Lectures will be held through Colab Notebooks. To download each notebook there are few and really simple steps to do:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNbGJa5Xkhb1",
        "colab_type": "text"
      },
      "source": [
        "## What is Natural Language Processing?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4_AIyXdjEob",
        "colab_type": "text"
      },
      "source": [
        "Natural Language Processing is a branch of computer science and artificial intelligence that enables computers to extract meaning from unstructured text.Computers are super-fast and super-good to process this kind of problem:\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "df = pd.read_csv(r'all_pubs_in_italy.csv')  \n",
        "df[df.menu.str.contains('spritz')]\n",
        "```\n",
        "\n",
        "but they are structured problems. Even if the computer is processing a text, somehow this text is not as natural as the Human language:\n",
        "\n",
        "`\"Ehi, ciao... mi diresti tutti i locali in cui posso bere uno spritz?\"`\n",
        "\n",
        "The language-processing capabilities of the human mind have been considered the most complex task for artificial intelligence. Human language is full of ambiguities.\n",
        "\n",
        "<center>  <img src=\"https://docs.google.com/uc?export=download&id=1BaKxrGQb_sPdtK-rFSNkrUv8e30P_4N4\" width=\"650\" height=\"400\"> </center> \n",
        "\n",
        "Moreover...\n",
        "\n",
        "- Language is highly ambiguous at all levels: \n",
        "\t- (Homophonic)  `l’amorale/la morale - hanno/anno - ai/hai` or `There is no right way to write a great novel`.\n",
        "\t- (Sintactic)  `Chiara ha visto Luca in giardino con il cannocchiale`, `Chiara saw Luca in the garden with a telescope.`\n",
        "\t- (Irony)  `Il mio volo è in ritardo. Splendido! `, `My flight is delayed. Wonderful! `\n",
        "- Humans often express their intent through semantically inaccurate language (dialects, sociolets, speech registers, errors when typing...)\n",
        "\n",
        "\n",
        "In addition, computers do not directly understand words. Classical approaches to NLP, were based on *rule-based AI* (*symbolic AI systems*) in which software engineers explicity specified the rules of parsing the meaning of language. This was labor-intensive and had limited application. \n",
        "\n",
        "The past few years NLP has been revolutionazed by **Deep Learning**!\n",
        "\n",
        "---\n",
        "\n",
        "Advances in the field of NLP have even paved the way for new applications. Here some of the most common examples:\n",
        "\n",
        "- Text Classification\n",
        "- Text Clustering\n",
        "- Text Summarisation\n",
        "- Machine Translation\n",
        "- Sentiment Analysis\n",
        "- Question Answering\n",
        "- Digital Assistant and Chatbot\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n8XiGYekdtn",
        "colab_type": "text"
      },
      "source": [
        "## Text Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWeQN6ugkjir",
        "colab_type": "text"
      },
      "source": [
        "For these lectures, I decided to put more emphasis on modern algorithms and tools, such as transformer-based models and Transformer libraries. However, one fundamental part of every machine learning project is data exploration and data cleaning. Feeding dirty data into a model will give us results that are meaningless. Keep in mind, \"*garbage in, garbage out*\". Especially in NLP, text preprocessing/cleaning can be very sophisticated and time-consuming. We will now briefly scatch some common tools.\n",
        "\n",
        "Here an example of an original review:\n",
        "\n",
        "```\n",
        "“I love my &lt;3 iphone &amp; you’re awsm apple. DisplayIsAwesome, sooo happppppy 🙂 http://www.apple.com”\n",
        "```\n",
        "\n",
        "We will now list some common steps\n",
        "\n",
        "**Common data cleaning steps:** \n",
        "\n",
        "Each of this step is not always needed!\n",
        "\n",
        "1. Make text all lower case \n",
        "2. Expand abbreviations\n",
        "3. Remove punctuation\n",
        "4. Remove numerical values or converting numbers into words\n",
        "5. Remove extra whitespace\n",
        "6. Remove stop words\n",
        "\n",
        "**More data preprocessing steps after tokenization:**\n",
        "\n",
        "7. Tokenize text\n",
        "8. Stemming / lemmatization\n",
        "9. Parts of speech tagging\n",
        "10. NER (Named Entity Recognition)\n",
        "11. Deal with typos\n",
        "\n",
        "After this celaning pipeline, text is still in word-form. However, computer understands only numbers. Therefore, before feeding a neural network with some text, we must perform a last step\n",
        "\n",
        "12. <font color='green'> Word Embedding </font> \n",
        "\n",
        "\n",
        "So let use start with some cleaning. We will take advantage just of **re** and **string** python packages. In my personal experience, I prefer to do all these steps with bash scripting (awk, sed, grep, ecc...) but for consistency here I listed them in python. Here you can find a [one-year old notebook](https://github.com/denocris/Deep-Learning-Lectures_NLP/blob/master/01_text_preprocessing.ipynb) where I show text cleaning also with bash tools."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agr2a80HoD23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "\n",
        "# Text Lowercase:\n",
        "# Lowercasing the text is used to reduce the size of the vocabulary of our text data, since it reduce text entropy.\n",
        "# 'Science' and 'science' became a single word to analyze.\n",
        "def text_lowercase(text):\n",
        "    return text.lower() \n",
        "\n",
        "# Remove punctuation:\n",
        "# String punctuation = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "def remove_punctuation(text): \n",
        "    no_punct_text = \"\".join([char for char in text if char not in string.punctuation])\n",
        "    return no_punct_text\n",
        "\n",
        "# Remove whitespaces -----------------------------------------------------------\n",
        "# Use the join and split function to remove all the white spaces in a string\n",
        "def remove_whitespace(text): \n",
        "    return  \" \".join(text.split()) \n",
        "\n",
        "\n",
        "def remove_unwanted_text(text):\n",
        "  new_text = str(text)  \n",
        "  new_text = re.sub('3+[0-9]{9}', '<mobilephone>', new_text) # adding <mobilphone> tag\n",
        "  new_text = re.sub('\\w+@\\w+\\.[a-z]{3}', '<email>', new_text) # adding <email> tag\n",
        "  new_text = new_text.replace('1st', 'first')\n",
        "  new_text = new_text.replace('2nd', 'second')  \n",
        "  new_text = re.sub('xké|xkè|xchè|xke|xche|perche|perché', 'perchè',new_text, flags=re.IGNORECASE)\n",
        "  #new_text = re.sub('xo|xò', 'però',new_text, flags=re.IGNORECASE) \n",
        "  return new_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj5nJZyNuelk",
        "colab_type": "code",
        "outputId": "0abc7da2-dbf8-4b79-c271-6ec42b5ad918",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "remove_unwanted_text('my 1st mail is denocris@gmail.com and my 2nd number 3234567890 xkè perche')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'my first mail is <email> and my second number <mobilephone> perchè perchè'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoYFYLXYVW4x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "c6439f9d-b663-41be-a1b8-dff2454a699f"
      },
      "source": [
        "import inflect #  generate plurals, singular nouns, ordinals, cardinals, indefinite articles; convert numbers to words\n",
        "p = inflect.engine() \n",
        "p.number_to_words(35) "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'thirty-five'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA8KTKivWR1J",
        "colab_type": "text"
      },
      "source": [
        "Cleaning a text takes a lot of time. Each text is different and you can choose to remove some steps or not. For instance, in many cases, capital letters are not relevant, but in others they are. In many cases, emoticons can be removed but if you are doing sentiment analysis they can be fundamental!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lrg8QCvkXmt",
        "colab_type": "text"
      },
      "source": [
        "## Text Preprocessing with spaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA_clsOD0NZm",
        "colab_type": "text"
      },
      "source": [
        "So far, we have been able to do text cleaning using only Python built-in modules like `re` and `string`. But, in order to perform next steps of pipeline, we are forced to import external open-source libraries, one of the most famous and used is spaCy. It is a free and open-source library for Natural Language Processing in Python with a lot of in-built capabilities. \n",
        "\n",
        "[spaCy](https://spacy.io/) provides a one-stop-shop for tasks commonly used in any NLP project, including:\n",
        "\n",
        "-    <font color='green'> Tokenization </font>  \n",
        "-    Lemmatization\n",
        "\n",
        "-    <font color='green'> Part-of-speech tagging </font>\n",
        "-    <font color='green'> Entity recognition </font>\n",
        "-    Dependency parsing\n",
        "-    Sentence recognition\n",
        "-    Word-to-vector transformations\n",
        "-    Many convenience methods for cleaning and normalising text\n",
        "\n",
        "\n",
        "Spacy releases general-purpose pretrained models to predict named entities, part-of-speech tags, syntactic dependencies and many others. Can be used out-of-the-box and fine-tuned on more specific data. Here you can find spaCy [Language Models](https://spacy.io/models)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nhQ4jF2hjm8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Let's load the English Language Model\n",
        "# It's the spacy nlp engine, it is initialized only once and then used for all spacy methods, in particular for POS-tag and NER. \n",
        "\n",
        "spacy_nlp = spacy.load('en_core_web_sm') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwbawFNt39dM",
        "colab_type": "text"
      },
      "source": [
        "#### Tokenization\n",
        "\n",
        "Tokenization is the process of splitting the given text into smaller pieces called tokens. They will be the **smallest units** that the algorithm will process. It is the most important brick in all preprocessing steps. Words, numbers, punctuation marks, and others can be considered as tokens. The most simple tokenization rule could be the presence of a whitespace\n",
        "\n",
        "`“Let’s have a spritz!” --->  ['\"Let's', 'go', 'to', 'N.Y.!\"']`\n",
        "\n",
        "More complex tokenization module, such as the spaCy one, separates word by space and then applying some guidelines such as exception rule, prefix, suffix. For instance:\n",
        "\n",
        "`“Let’s have a spritz!” --->  ['Let', ''', 's', 'have', 'a', 'spritz\", '!\"']`\n",
        "\n",
        "BERT-like models (we will see) use a different kind of tokenizer, the so called word-piece tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvmj-AtUaMeD",
        "colab_type": "code",
        "outputId": "88c3cfa2-ff24-4a0d-9310-c35ac6b662cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "doc = spacy_nlp('Where are we heading?')\n",
        "\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Where', 'are', 'we', 'heading', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9W0DCK4bdwT",
        "colab_type": "text"
      },
      "source": [
        "#### Part Of Speech (POS) Tagging\n",
        "This step and the next one we are going to see are fully part of preprocessing pipeline but, in addition, the can be see as *Information Extraction* and *Text Data Augmentation* steps. \n",
        "\n",
        "These two steps are of fundamental importance for the understanding of the semantic and lexical structure of text. \n",
        "\n",
        "Part-of-speech tagging aims to assign parts of speech to each word of a given text (such as nouns, verbs, adjectives, and others) based on its definition and its context.\n",
        "\n",
        "![alt text](https://1.bp.blogspot.com/-spGNcdlw7g4/XHY5fS25uVI/AAAAAAAABqY/63lfyQFHkl4rf1ls0vvLIBRRc8TEsBZvgCLcBGAs/s640/Capture.PNG)\n",
        "\n",
        "Part of speech general tags: Noun (N), Verb (V), Adjective(ADJ), Adverb (ADV), Preposition (P), Conjunction (CON), Pronoun(PRO), Interjection (INT)\n",
        "\n",
        "POS tagging is one of the fundamental tasks of natural language processing. For instance, it is the basis of **Word-Sense Disambiguation**.\n",
        "\n",
        "-> `Ci sei (VERB) alle sei (NUMBER)?`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0To3VKSeWYG",
        "colab_type": "code",
        "outputId": "c0fc861d-5b63-4d0e-c1a0-458f27c95862",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        }
      },
      "source": [
        "doc = spacy_nlp(\"Can you please buy me a Sprits? It's 3.50 euros.\")\n",
        "for token in doc:\n",
        "   print(token.text, token.lemma_, token.pos_)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Can Can VERB\n",
            "you -PRON- PRON\n",
            "please please INTJ\n",
            "buy buy VERB\n",
            "me -PRON- PRON\n",
            "a a DET\n",
            "Sprits Sprits PROPN\n",
            "? ? PUNCT\n",
            "It -PRON- PRON\n",
            "'s be AUX\n",
            "3.50 3.50 NUM\n",
            "euros euro NOUN\n",
            ". . PUNCT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGz-UUqddX8m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "outputId": "98681e05-7af9-4ec1-ac56-8a6c211846c1"
      },
      "source": [
        "from spacy import displacy\n",
        "displacy.render(doc, style='dep', jupyter=True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"9f98349bbc324c2c958b09a01625f599-0\" class=\"displacy\" width=\"1975\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Can</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">you</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">please</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">INTJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">buy</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">me</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">a</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Sprits?</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">It</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">'s</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">3.50</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NUM</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">euros.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9f98349bbc324c2c958b09a01625f599-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,2.0 575.0,2.0 575.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9f98349bbc324c2c958b09a01625f599-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9f98349bbc324c2c958b09a01625f599-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9f98349bbc324c2c958b09a01625f599-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9f98349bbc324c2c958b09a01625f599-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9f98349bbc324c2c958b09a01625f599-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">intj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9f98349bbc324c2c958b09a01625f599-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9f98349bbc324c2c958b09a01625f599-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dative</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M740.0,266.5 L748.0,254.5 732.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9f98349bbc324c2c958b09a01625f599-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9f98349bbc324c2c958b09a01625f599-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9f98349bbc324c2c958b09a01625f599-0-5\" stroke-width=\"2px\" d=\"M595,264.5 C595,2.0 1100.0,2.0 1100.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9f98349bbc324c2c958b09a01625f599-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1100.0,266.5 L1108.0,254.5 1092.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9f98349bbc324c2c958b09a01625f599-0-6\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,177.0 1440.0,177.0 1440.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9f98349bbc324c2c958b09a01625f599-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1295,266.5 L1287,254.5 1303,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9f98349bbc324c2c958b09a01625f599-0-7\" stroke-width=\"2px\" d=\"M1645,264.5 C1645,177.0 1790.0,177.0 1790.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9f98349bbc324c2c958b09a01625f599-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1645,266.5 L1637,254.5 1653,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9f98349bbc324c2c958b09a01625f599-0-8\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,89.5 1795.0,89.5 1795.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9f98349bbc324c2c958b09a01625f599-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1795.0,266.5 L1803.0,254.5 1787.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH_kCJmzkGYd",
        "colab_type": "text"
      },
      "source": [
        "#### Named Entity Recognition (NER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-8t1Hulhwir",
        "colab_type": "text"
      },
      "source": [
        "NER, short for Named Entity Recognition is probably the first step towards **Information Extraction from unstructured text**. It basically means extracting what is a real world entity from the text (Person,Organization, Event etc)\n",
        "\n",
        "![alt text](https://miro.medium.com/max/890/1*9ICjpdIPiocUWC5oF47duw.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JzWzI80koyq",
        "colab_type": "code",
        "outputId": "cddef173-70dc-4249-8e2b-c5e9a78bc1f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "text = \"\"\"MHPC is an innovative specialization program that prepares students for exciting careers \n",
        "in the fast-growing field of high-performance computing. Among many, there is an interesting course on \n",
        "Deep Learning by Alessio Ansuini, Cristiano De Nobili, and Piero Coronica.\n",
        "\n",
        "\"\"\"\n",
        "doc = spacy_nlp(text)\n",
        "\n",
        "for entity in doc.ents:\n",
        "    #print(spacy.explain(entity.label_))\n",
        "    spacy_expl=spacy.explain(entity.label_)\n",
        "    print(f\"{entity.text} ({entity.label_} : {spacy_expl} )\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MHPC (ORG : Companies, agencies, institutions, etc. )\n",
            "Deep Learning (WORK_OF_ART : Titles of books, songs, etc. )\n",
            "Alessio Ansuini (PERSON : People, including fictional )\n",
            "Cristiano De Nobili (PERSON : People, including fictional )\n",
            "Piero Coronica (ORG : Companies, agencies, institutions, etc. )\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF9WkgTTc1Vt",
        "colab_type": "text"
      },
      "source": [
        "Always pay attention to pre-trained models! In most of the cases, it is always necessary a fine-tuning of them on your specific dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUKrZzhvnSBF",
        "colab_type": "code",
        "outputId": "e741cb34-03e6-45a1-f1f8-ae3f1713cc98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "displacy.render(doc, style='ent', jupyter=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    MHPC\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is an innovative specialization program that prepares students for exciting careers </br>in the fast-growing field of high-performance computing. Among many, there is an interesting course on </br>\n",
              "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Deep Learning\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
              "</mark>\n",
              " by \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Alessio Ansuini\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Cristiano De Nobili\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", and \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Piero Coronica\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ".\n",
              "\n",
              "</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WyqkEFmFj2O",
        "colab_type": "text"
      },
      "source": [
        "## Word Embedding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PFk4etA44Xa",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "NLP is hard for several reasons, including that it handles **non-numerical data**: a text is represented as a sequence of chars, which is not suitable for neural networks. \n",
        "\n",
        "For this reason, it is necessary **a transformation from text to numerical values which can as much as possible keep the meaning of the word**, without losing all that which the word evokes in the human mind. \n",
        "\n",
        "This transformation is known as **Word Embedding**. We can go to a lower level of analysis and create a *Characters Embedding*, or higher and have a *Sentence Embedding*. This depends on which tokens (our atomic elements) we chose.\n",
        "\n",
        ">  —  Word embedding is the collective name for a set of language modelling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. — (Wikipedia)\n",
        "\n",
        "The best approach so far, among many, has been to map words to vectors. This allows us to endorse a word with many degrees of freedom. Moreover, once trained, an algorithm must be able to rotate random initialized vectors according to their similarity. Therefore, similar words can be placed nearby in vector space.\n",
        "\n",
        "\n",
        "<center>  <img src=\"https://docs.google.com/uc?export=download&id=1rbEhHbP8WtJ0HsKzJlXYy6K2-MHvVqU6\" width=\"650\" height=\"400\"> </center> \n",
        "\n",
        "\n",
        "There are two main Word Embeddings:\n",
        "\n",
        "-    <font color='green'> Non-contextual word embeddings </font>  \n",
        "-    <font color='green'> Contextual word embeddings </font> \n",
        "\n",
        "\n",
        "Here, a short list of most influential embedding algorithms of last years, each of these has bees state-of-the-art algoritms:\n",
        "\n",
        "- **Word2Vec** ([Mikolov et al., 2013](https://arxiv.org/abs/1301.3781));\n",
        "- **Global Vectors for Word Representation** (GloVe)  ([Pennington et al., 2014](https://www-nlp.stanford.edu/pubs/glove.pdf));\n",
        "- **FastText** ( [Mikolov et al., 2016](https://arxiv.org/abs/1607.04606));\n",
        "- **Embeddings from Language Models** (ELMo) ([Peters et al., 2018](https://arxiv.org/abs/1802.05365));\n",
        "\n",
        "... and based on transformers:\n",
        "- **Attention is all you need** (Transformers)  ([Vaswani et al., 2017 ](https://arxiv.org/abs/1706.03762));\n",
        "- **Generative Pre-Training** (GPT) ([Radford et al., 2018](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf));\n",
        "- **Bidirectional Encoder Representations from Transformers** (BERT)  ([Devlin et al., 2018](https://arxiv.org/abs/1810.04805))\n",
        "- **Others**...\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSe2tzw5FgaG",
        "colab_type": "text"
      },
      "source": [
        "## Non-Contextual Word Embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4PEN5khNR4m",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The main idea behind most of the NLP algorithms is the representation of a token (characters, words, or sentences) in terms of vectors in a high-dimensional space. Once this conversion is achieved, many linear algebra tools can be applied to further investigations.\n",
        "\n",
        "For the moment we will limit to **non-contextual embedding** (we will see contextual embedding later.). Regardless of multiple meanings of a given word, it will be represented by one vector. For instance, the word `second` in English will have the same vector even if its meaning can be different:\n",
        "\n",
        "`This is the second time that I try it`\n",
        "\n",
        "`He just arrived one second later`\n",
        "\n",
        "### Main idea\n",
        "\n",
        "In 2013, with [Word2Vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) , Mikolov et al. at Google completely changed the embedding paradigm: from then on, embedding will be the weights of a neural network that are adjusted to minimize some loss, depending on the task. **Embedding had become a neural network algorithm.**\n",
        "\n",
        "Now, let us take a very simple example to understand why the approach of considering words as vectors is so successful. Suppose to train a black-box algorithm (we will see in detail it later) that gives you back the following **trained vectors** for the words Trieste, sea, and Milan."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvF0jaYXrw9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# vector representing the word 'Trieste'\n",
        "vec1_trieste= np.array([0.9, 0.82, 0.75])\n",
        "\n",
        "# vector representing the word 'sea'\n",
        "vec2_sea = np.array([0.5, 0.98, 0.92])\n",
        "\n",
        "# vector representing the word 'Milan'\n",
        "vec3_milan = np.array([0.91, 0.11, 0.25])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGsBtf25xhe7",
        "colab_type": "text"
      },
      "source": [
        "Now we can measure the similarity between vectors taking advantage of the (normalized) scalar product, often named cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luooX67XxkrA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cosine_similarity(w1, w2):\n",
        "  return np.dot(w1,w2)/(np.dot(w1,w1)*np.dot(w2,w2))**0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrOppAqcxnz8",
        "colab_type": "code",
        "outputId": "a06946d9-2874-49a9-dde5-51744c49e949",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "print('Similarity between Trieste & Sea:   ', cosine_similarity(vec1_trieste,vec2_sea))\n",
        "\n",
        "print('Similarity between Trieste & Milan: ', cosine_similarity(vec1_trieste,vec3_milan))\n",
        "\n",
        "print('Similarity between Milan & Sea:    ', cosine_similarity(vec3_milan, vec2_sea))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similarity between Trieste & Sea:    0.9477085988044828\n",
            "Similarity between Trieste & Milan:  0.807198014124129\n",
            "Similarity between Milan & Sea:     0.5818297027006112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH3CaSHizdqQ",
        "colab_type": "text"
      },
      "source": [
        "**Key point**: how to find out the best vector representation of a give word?\n",
        "\n",
        "That's about choosing the proper algorithm that it is able to process the input text and output the best representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USTpjSHcj3bA",
        "colab_type": "text"
      },
      "source": [
        "### Word2Vec Neural Network (code it from scratch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K88XqXECTV1f",
        "colab_type": "text"
      },
      "source": [
        "Inspired by [this blog](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/):\n",
        "\n",
        "* We will code from scratch the skip-gram neural network architecture for [Word2Vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). The skip-gram neural network model is surprisingly simple in its most basic form;\n",
        "\n",
        "* The goal is to understand its principle and recap some basics understanding of deep learning using PyTorch.\n",
        "\n",
        "We will train a simple neural network with a single hidden layer to perform a certain task, but then we are not actually going to use that neural network for the task we trained it on! Instead, the goal is just to learn the weights of the hidden layer (Somehow this is similar to autoencoders goal). At the end, we will see that these weights are the *word vectors* that we are trying to learn.\n",
        "\n",
        "We will train the NN to do the following. Given a specific word in the middle of a sentence (the input word), look at the words nearby (window size) and pick one at random. Once trained, the NN will tell us the probability for every word in our vocabulary of being the nearby word that we choose.\n",
        "\n",
        "For instance, if we gave the trained network the input word *Friuli*, the output probabilities will be higher for words like *Venezia* and *Giulia* than *spaghetti* and *mandolino*.\n",
        "\n",
        "Word2Vec can be structured using two different approach:\n",
        "\n",
        "* CBOW: The neural network takes a look at the surrounding words and predicts the word that comes in between.\n",
        "\n",
        "* SkipGram: The neural network takes in a word and then tries to predict the surrounding words.\n",
        "\n",
        "We will code SkipGram."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zxA3iYHQ0sh",
        "colab_type": "code",
        "outputId": "b2fd5039-0ca2-4131-8fec-963bb6cdb6e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dtype = torch.FloatTensor\n",
        "\n",
        "# 3 Words Sentence (to semplify)\n",
        "# All them form our text corpus\n",
        "sentences = [ \"i like dog\", \"i like cat\", \"i like animal\",\n",
        "              \"dog cat animal\", \"apple cat dog like\", \"dog fish milk like\",\n",
        "              \"dog cat eyes like\", \"i like apple\", \"apple i hate\",\n",
        "              \"apple i movie\", \"book music like\", \"cat dog hate\", \"cat dog like\"]\n",
        "\n",
        "# list all the words present in our corpus\n",
        "word_sequence = \" \".join(sentences).split()\n",
        "print(word_sequence )\n",
        "# build the vocabulary\n",
        "word_list = list(set(word_sequence))\n",
        "print(word_list)\n",
        "word_dict = {w: i for i, w in enumerate(word_list)}\n",
        "print(word_dict)\n",
        "\n",
        "# Word2Vec Parameter\n",
        "batch_size = 20  # To show 2 dim embedding graph\n",
        "embedding_size = 2  # To show 2 dim embedding graph\n",
        "voc_size = len(word_list)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'like', 'dog', 'i', 'like', 'cat', 'i', 'like', 'animal', 'dog', 'cat', 'animal', 'apple', 'cat', 'dog', 'like', 'dog', 'fish', 'milk', 'like', 'dog', 'cat', 'eyes', 'like', 'i', 'like', 'apple', 'apple', 'i', 'hate', 'apple', 'i', 'movie', 'book', 'music', 'like', 'cat', 'dog', 'hate', 'cat', 'dog', 'like']\n",
            "['apple', 'fish', 'book', 'i', 'like', 'eyes', 'dog', 'hate', 'movie', 'milk', 'animal', 'music', 'cat']\n",
            "{'apple': 0, 'fish': 1, 'book': 2, 'i': 3, 'like': 4, 'eyes': 5, 'dog': 6, 'hate': 7, 'movie': 8, 'milk': 9, 'animal': 10, 'music': 11, 'cat': 12}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1V7UxOXNhurO",
        "colab_type": "code",
        "outputId": "afdf2fcd-d203-4ca8-9d9a-b0532c092fc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# input word\n",
        "i = 1 \n",
        "print(word_sequence[i], word_dict[word_sequence[i]])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "like 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__-rG0E2isup",
        "colab_type": "code",
        "outputId": "8828eaec-abf5-4267-c444-d866a87540df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# context words\n",
        "print(word_sequence[i - 1], word_sequence[i + 1])\n",
        "print([word_dict[word_sequence[i - 1]], word_dict[word_sequence[i + 1]]])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i dog\n",
            "[3, 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbURiETUoDU4",
        "colab_type": "text"
      },
      "source": [
        "Chosen word `like`, context words `i` and `dog`: <font color='green'> i </font>  like <font color='green'> dogs </font> \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fusNAmdNhEKe",
        "colab_type": "code",
        "outputId": "2d256671-88aa-4de4-a3bb-4a60aabb0fec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Make skip gram of one size window\n",
        "skip_grams = []\n",
        "for i in range(1, len(word_sequence) - 1):\n",
        "    target = word_dict[word_sequence[i]]\n",
        "    context = [word_dict[word_sequence[i - 1]], word_dict[word_sequence[i + 1]]]\n",
        "\n",
        "    for w in context:\n",
        "        skip_grams.append([target, w])\n",
        "\n",
        "skip_grams[:6]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[4, 3], [4, 6], [6, 4], [6, 3], [3, 6], [3, 4]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFjKmuGYkXBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# np.eye() Return a 2-D array with ones on the diagonal and zeros elsewhere.\n",
        "np.eye(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVDRWdiDhF9N",
        "colab_type": "code",
        "outputId": "a27adde3-bbb0-4df1-a610-d1ec9a1d4727",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "np.random.seed(172)\n",
        "\n",
        "def random_batch(data, size):\n",
        "    random_inputs = []\n",
        "    random_labels = []\n",
        "    random_index = np.random.choice(range(len(data)), size, replace=False)\n",
        "\n",
        "    for i in random_index:\n",
        "        # one-hot encoding of words\n",
        "        random_inputs.append(np.eye(voc_size)[data[i][0]])  # input\n",
        "        random_labels.append(data[i][1])  # context word\n",
        "\n",
        "    return random_inputs, random_labels\n",
        "\n",
        "random_batch(skip_grams[:6], size=3)\n",
        "\n",
        "# inputs: like , i, dog , context: i, dog, i"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              "  array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              "  array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])],\n",
              " [3, 6, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTkk4VUP4s6A",
        "colab_type": "text"
      },
      "source": [
        "Here a scatch of our model. There is no activation function on the hidden layer neurons, but the output neurons use softmax. \n",
        "\n",
        "![alt text](http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNIS3qRdm_Lx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model\n",
        "class Word2Vec(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Word2Vec, self).__init__()\n",
        "\n",
        "        # parameters between -1 and + 1\n",
        "        self.W = nn.Parameter(-2 * torch.rand(voc_size, embedding_size) + 1).type(dtype) # voc_size > embedding_size Weight\n",
        "        self.V = nn.Parameter(-2 * torch.rand(embedding_size, voc_size) + 1).type(dtype) # embedding_size > voc_size Weight\n",
        "\n",
        "    def forward(self, X):\n",
        "        # X : [batch_size, voc_size], self.W: [voc_size, embedding_size]\n",
        "        hidden_layer = torch.matmul(X, self.W) # hidden_layer : [batch_size, embedding_size]\n",
        "        # self.V: [voc_size, embedding_size]\n",
        "        output_layer = torch.matmul(hidden_layer, self.V) # output_layer : [batch_size, voc_size]\n",
        "        return output_layer\n",
        "\n",
        "model = Word2Vec()\n",
        "# Set the model in train mode\n",
        "model.train()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() # Softmax (for multi-class classification problems) is already included\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4iP6LaT079O",
        "colab_type": "text"
      },
      "source": [
        "<center>  <img src=\"https://docs.google.com/uc?export=download&id=1gl-0b4kQCuj68Ww43k6EVoTL2xfJjbXH\" width=\"650\" height=\"400\"> </center> \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_TwkQygquXz",
        "colab_type": "code",
        "outputId": "8492432e-6af2-40f8-92f6-5bac92948e44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "# Training\n",
        "for epoch in range(5000):\n",
        "\n",
        "    input_batch, target_batch = random_batch(skip_grams, batch_size)\n",
        "\n",
        "    # new_tensor(data, dtype=None, device=None, requires_grad=False)\n",
        "    input_batch = torch.Tensor(input_batch)\n",
        "    target_batch = torch.LongTensor(target_batch)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output = model(input_batch)\n",
        "\n",
        "    # output : [batch_size, voc_size], target_batch : [batch_size] (LongTensor, not one-hot)\n",
        "    loss = criterion(output, target_batch)\n",
        "    if (epoch + 1)%1000 == 0:\n",
        "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1000 cost = 2.177021\n",
            "Epoch: 2000 cost = 2.255151\n",
            "Epoch: 3000 cost = 1.919636\n",
            "Epoch: 4000 cost = 2.194563\n",
            "Epoch: 5000 cost = 1.871328\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LHlIKKfhUiV",
        "colab_type": "text"
      },
      "source": [
        "Now that the training has just finished, we can extract the **learned embeddings**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPGrLtBtIOb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Learned W\n",
        "W, _= model.parameters()\n",
        "print(W)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icd0VwtrwRBL",
        "colab_type": "code",
        "outputId": "4288bf8c-9927-4bc6-fd62-e77937d24916",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "for i, word in enumerate(word_list):\n",
        "    W, _= model.parameters()\n",
        "    x,y = float(W[i][0]), float(W[i][1])\n",
        "    plt.scatter(x, y)\n",
        "    plt.annotate(word, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
        "plt.show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSxQyMcJ5NmF",
        "colab_type": "text"
      },
      "source": [
        "Ok, this plot and embeddings make no sense. This was just a simple example. Its aim was just to show you how to build from scratch a Word2vec. Now we will use an already (well) trained embedding..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nUazbJq1R9Q",
        "colab_type": "text"
      },
      "source": [
        "### Pre-trained (non-contextual) Word Embedding - Glove\n",
        "\n",
        "[GloVe](https://nlp.stanford.edu/projects/glove/) is an unsupervised learning algorithm for obtaining vector representations for words. \n",
        "\n",
        "Now, using [gensim](https://github.com/RaRe-Technologies/gensim/#documentation), we download a pre-trained English embedding that was trained on Wikipedia and others open datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzivCJZM3G8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim.downloader as api\n",
        "word_vectors = api.load(\"glove-wiki-gigaword-100\")  # load pre-trained word-vectors from gensim-data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53hNkDCS50lg",
        "colab_type": "text"
      },
      "source": [
        "Given a word (i.e. a vector), we can find its most similar word (i.e. closest vectors)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqZsfJV85zb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_vectors.most_similar('airplane'), word_vectors.most_similar('sea')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P02favbA5414",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_vectors.most_similar('second'), word_vectors.most_similar('lie')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr6VnCmC599Z",
        "colab_type": "text"
      },
      "source": [
        "Once a word is a vector, not only you can compute similarities between words but also operations. Let's have a look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK12w-EL6ElZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = word_vectors.most_similar(positive=['helicopter', 'wings'])\n",
        "print(\"{}: {:.4f}\".format(*result[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjxhOxOJ6I2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = word_vectors.most_similar(positive=['plane'], negative=['wings'])\n",
        "print(\"{}: {:.4f}\".format(*result[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGACzUOV6QHh",
        "colab_type": "text"
      },
      "source": [
        "And again..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eQeZPRk6LpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])\n",
        "print(\"{}: {:.4f}\".format(*result[0]))\n",
        "\n",
        "result = word_vectors.most_similar(positive=['man', 'queen'], negative=['woman'])\n",
        "print(\"{}: {:.4f}\".format(*result[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyUTMlXv6U6x",
        "colab_type": "text"
      },
      "source": [
        "look this..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOT81NM86WF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = word_vectors.most_similar(positive=['thames', 'paris'], negative=['london'])\n",
        "print(\"{}: {:.4f}\".format(*result[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCaK8Ud06a9O",
        "colab_type": "text"
      },
      "source": [
        "*Find* the intruder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jBaRrYK6Y1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(word_vectors.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n",
        "print(word_vectors.doesnt_match(\"obama kennedy trump dylan\".split()))\n",
        "print(word_vectors.doesnt_match(\"paris milano moscow hamburg madrid cake\".split()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO8d1MqVFZX5",
        "colab_type": "text"
      },
      "source": [
        "## Contextual Embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6G64dxRRxZf",
        "colab_type": "text"
      },
      "source": [
        "To have a wider overview I suggest you this slide from [Stanford Edu](https://web.stanford.edu/class/cs224n/slides/Jacob_Devlin_BERT.pdf). They are about Pre-trained Contextual Embedding history and zoology.\n",
        "\n",
        "Previously, we associated a vector at each word. The correspondence was one-to-one. However, words can have different meanings in different contexts.\n",
        "\n",
        "Thus, we need an embedding mechanism that can understand a word in its context.\n",
        "The main goal of contextual word embedding is to try to capture a word token’s meaning primarily through the specific context it appears in. This means that every instance of *secondo* will have a different word vector; those with a context that looks similar are expected to be close to each other, while those with a contexts that looks different will cluster elsewhere in vector space.\n",
        "\n",
        "<center>  <img src=\"https://docs.google.com/uc?export=download&id=1RkOQccAo00Q-KU3ROk5oe3PcnqlAx99u\" width=\"650\" height=\"400\"> </center> \n",
        "\n",
        "The topic of contextual embedding is very interesting and recent (a bit more than a couple of years!). Despite its young age, in these two years, lots of papers and models have been produced. The fundamental unit of state-of-the-art models, called **Transformer architecture**, is too complicated to code from scratch in a limited time.\n",
        "\n",
        "Therefore, in these lectures (Next Notebook) we will limit ourself to:\n",
        "\n",
        "- Have an intuition about the basic mechanism behind Transformers: The Attention Mechanism\n",
        "\n",
        "- Intruducing BERT-like models and use them as a tool to solve a complex task.\n",
        "\n",
        ">  —  The reasons for BERT's state-of-the-art performance on these natural language understanding tasks are not yet well understood. — (Wikipedia)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gVgjmtBQ5nS",
        "colab_type": "text"
      },
      "source": [
        "### The Self-Attention Mechanism\n",
        "\n",
        "The Attention Mechanism, especially self-attention, is the main idea behind recent outstanding progress in NLP (and not only!). In particular, it is the core idea behind the Transformer architecture, first exposed in this revolutionary paper [Vaswani & al., 2017](https://arxiv.org/abs/1706.03762) by Google Brain.\n",
        "\n",
        "One advantage of Transformer over its RNN counterpart (previous state-of-the-art model) it its non-sequential attention model. Remember, the RNNs has to\n",
        "iterate over each element of the input sequence one-by-one and carry an \"updatable-state\" between each hop. With a Transformer, the model is able to look at every position in the sequence, at the same time, in one operation.\n",
        "\n",
        "To have an intuitive idea about we will go through these [slides](https://docs.google.com/presentation/d/1HVAYCflD-KlQ6LOpkPCEqd-LVnx8gyIY6D1hykKvhCU/edit?usp=sharing).\n",
        "\n",
        "\n",
        "<center>  <img src=\"https://docs.google.com/uc?export=download&id=1cujjCUV98lgKX6Qv13G2F6s5LTwkod5K\" width=\"600\" height=\"600\"> </center> "
      ]
    }
  ]
}